{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n",
      "Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\n",
      "Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio\n",
      "OS in my system :  win32\n",
      "\n",
      "Bendras duomenų aplankas:  D:\\DI\\Data\\MIT&ZIVE\\VU\n",
      "Zive duomenų aplankas:  DUOM_VU\n",
      "Aplankas su originaliais EKG įrašais ir anotacijomis (.json)  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\n",
      "Pūpsnių atributų failas: all_beats_attr_z.csv\n",
      "Diskretizavimo dažnis:  200\n",
      "Klasifikavimo schema: {'N': 0, 'S': 1, 'V': 2}\n",
      "Klasių skaičius: 3\n",
      "Modelio ir scaler parametrai nuskaitomas iš aplanko:  model_cnn_fda_vu_v1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Skrtas masiniam testavimui - variantas su model_cnn_fda_vu_v1\n",
    "\n",
    "# Perdarytas iš 6_zive_accuracy_cnn_vu_v2_test_batch.ipynb:\n",
    "# - pritaikytas variantui, kai klasifikacija izoliuota vienam pūpsniui\n",
    "# - pakeistas naudojamų požymių sąrašas (pritaikytas vasariniam modeliui 1)\n",
    "# - naudoja požymių skaičiavimą iš zive_cnn_fda_vu_v3_micro.py (from zive_cnn_fda_vu_v3_micro import predict_cnn_fda_vu_v1),\n",
    "# kurį gavau iš Povilo 2022 10 05 - faile ReadSeqEKG_2022_10_05.py: get_spike_width funkcija keičiasi,\n",
    "#  keičiasi ir apply_FDA (perpavardinau į get_beat_features_set_fda_vu_v1)\n",
    "# \n",
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šis variantas pritaikytas npy formato zive įrašams, kuriems pakeistas, lyginant su \n",
    "# originaliais įrašais, failo vardas iš `file_name` į `SubjCode`, pridedant `userNr`\n",
    "# prie `file_name`. \n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos \n",
    "# paimamos iš aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys\n",
    "# iš db_folder įrašų saugyklos, jame yra ir failas all_beats_attr. \n",
    "\n",
    "# Testavimui imami įrašai iš sąrašo SubjCodes, kuris arba paimamas if failo info_create.json,\n",
    "# arba iš mokymo, validavimo, testavimo sarašų, pvz. train_subjcode_lst.csv. Visiems įrašams iš šių\n",
    "# sąrašų egzistuoja informacija apie pūpsnius faile all_beats_attr.\n",
    " \n",
    "# Skripte yra galimybė išvesti ekstrasistolių vietas įraše.\n",
    "# Dirbant su daug įrašų reiktų užblokuoti: classification = []  # Užblokuota\n",
    " \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "# import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys, os, json\n",
    "from pathlib import Path\n",
    "# from icecream import ic\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from zive_util_vu import cm2df, show_confusion_matrix \n",
    "from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "from zive_util_vu import runtime, split_SubjCode\n",
    "from zive_util_vu import get_freq_unique_values\n",
    "\n",
    "from zive_util_vu import get_beat_attributes\n",
    "from zive_util_vu import get_userId, read_rec, get_filename \n",
    "from zive_util_vu import confusion_matrix_modified, zive_read_df_rpeaks\n",
    "\n",
    "from zive_cnn_fda_vu_v3_micro import get_beat_features_fda_vu_v1_vasara\n",
    "from zive_cnn_fda_vu_v3_micro import zive_read_file_1ch, read_df_rpeaks\n",
    "# Pastaba: zive_read_file_1ch importuoju iš zive_cnn_fda_vu_v1, nors ji yra ir zive_util_vu.py\n",
    "# tam, kad atskirti funkcijas, kurios importuojamos skripte zive analysis, nuo tų funkcijų,\n",
    "# kurios reikalingos tik zive_accuracy_cnn_vu_v1 ir v2. \n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\")\n",
    "print('Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "    # Duomenu_aplankas = 'F:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Herkulis\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "all_beats_attr_fname = 'all_beats_attr_z.csv'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3, 'F':3}  \n",
    "\n",
    "# Diskretizavimo dažnis:\n",
    "fs = 200\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_npy')\n",
    "\n",
    "# Nuoroda į modelio aplanką\n",
    "# model_dir = Path(Duomenu_aplankas, 'DNN', 'best_models', 'all_ft')\n",
    "model_dir = 'model_cnn_fda_vu_v1'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su originaliais EKG įrašais ir anotacijomis (.json) \", rec_dir)\n",
    "print(\"Pūpsnių atributų failas:\", all_beats_attr_fname)\n",
    "print(\"Diskretizavimo dažnis: \", fs)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "print(\"Modelio ir scaler parametrai nuskaitomas iš aplanko: \", model_dir)\n",
    "\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Atliekama pūpsnių pacientų įrašuose klasifikacija\n",
      "Klasifikuojamų įrašų sąrašas: [10022, 10021, 10083, 10091]\n",
      "Sąrašas nuskaitytas iš: D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\\test_subjcode_lst.csv\n",
      "\n",
      "Aplankas rezultatams: D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_test_subjcode_lst\n",
      "Directory 'D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_test_subjcode_lst' already exists\n",
      "\n",
      "SubjCode: 10022 userNr: 1002 file_name: 1625400.796 signal_length: 127999\n",
      "test_labels:  [0 1 2] [802   2  14] 818\n",
      "pred_labels:  [0 2 3] [806   9   3] 818\n",
      "N:  799 S:  2 V: 14  Nprec: 0.99 Nrec: 1.00 Nfsc: 1.00  Sprec: 0.00 Srec: 0.00 Sfsc: 0.00  Vprec: 0.89 Vrec: 0.57 Vfsc: 0.70\n",
      "\n",
      "SubjCode: 10021 userNr: 1002 file_name: 1625402.027 signal_length: 127999\n",
      "test_labels:  [0 1 2] [748   4  13] 765\n",
      "pred_labels:  [0 1 2 3] [753   2   5   5] 765\n",
      "N:  744 S:  4 V: 12  Nprec: 0.99 Nrec: 1.00 Nfsc: 0.99  Sprec: 0.50 Srec: 0.25 Sfsc: 0.33  Vprec: 0.80 Vrec: 0.33 Vfsc: 0.47\n",
      "\n",
      "SubjCode: 10083 userNr: 1008 file_name: 1630757.924 signal_length: 127999\n",
      "test_labels:  [0 1 2] [723  11   8] 742\n",
      "pred_labels:  [0 2 3] [722   9  11] 742\n",
      "N:  715 S: 11 V:  5  Nprec: 0.99 Nrec: 1.00 Nfsc: 1.00  Sprec: 0.00 Srec: 0.00 Sfsc: 0.00  Vprec: 0.56 Vrec: 1.00 Vfsc: 0.71\n",
      "\n",
      "SubjCode: 10091 userNr: 1009 file_name: 1631141.764 signal_length: 127999\n",
      "test_labels:  [0 1] [845  20] 865\n",
      "pred_labels:  [0 1 2 3] [805  43  15   2] 865\n",
      "N:  843 S: 20 V:  0  Nprec: 1.00 Nrec: 0.95 Nfsc: 0.97  Sprec: 0.12 Srec: 0.25 Sfsc: 0.16  Vprec: 0.00 Vrec: 0.00 Vfsc: 0.00\n",
      "\n",
      "\n",
      "Runtime: 00:04:32\n"
     ]
    }
   ],
   "source": [
    "# Pagrindinis skriptas\n",
    "\n",
    "\n",
    "def get_beat_features_set_fda_vu_v1_micro(signal, atr_sample, idx_lst):\n",
    "# Apskaičiuojami užduotų EKG signalo pūpsnių (per idx_lst) požymiai ir iš jų\n",
    "# suformuojamas požymių dataframe masyvas, pridedant 'label'\n",
    "\n",
    "    #  all_beats pritaikytas MIT2ZIVE ir ZIVE duomenims\n",
    "    all_beats = {'N':0,'R':0, 'L':0, 'e':0, 'j':0, 'A':1,'a':1, 'J':1, 'S':1, 'V':2, 'E':2, 'F':3, 'U':3, 'Q':3}\n",
    "\n",
    "    beat_features_set = pd.DataFrame()\n",
    "    omit_idx_set = pd.DataFrame()\n",
    "\n",
    "    # atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    # atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    # test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "    \n",
    "    # print(\"\\nGet beat features set from signal:\")\n",
    "    for idx in idx_lst:\n",
    "        beat_features, omit_idx = get_beat_features_fda_vu_v1_vasara(signal, atr_sample, idx)\n",
    "        # beat_features_set = beat_features_set.append(beat_features, ignore_index=True)\n",
    "        if omit_idx.empty:\n",
    "            # beat_features['label'] = test_labels[idx]\n",
    "            beat_features_set = pd.concat([beat_features_set, beat_features])\n",
    "        else:\n",
    "            omit_idx_set = pd.concat([omit_idx_set, omit_idx])\n",
    "\n",
    "    # Konvertuojame int pozymius į float64\n",
    "    beat_features_set['RR_l_0'] = beat_features_set['RR_l_0'].astype(float)\n",
    "    beat_features_set['RR_r_0'] = beat_features_set['RR_r_0'].astype(float)\n",
    "\n",
    "    return beat_features_set, omit_idx_set\n",
    "\n",
    "\n",
    "def predict_cnn_fda_vu_v1_micro(signal, atr_sample, model_dir):\n",
    "# ************************************* funkcijai ***************************************************************\n",
    "\n",
    "#  Iėjimo parametrai:   signal, atr_sample, \n",
    "#                       modelio ir scaler_train parametrai\n",
    "#                       \n",
    "#  Išėjimo parametrai: pred_y, kai kurie neatpažinti : 'U':3\n",
    "\n",
    "    # Suformuojame indeksų sąrašą. Formuodami sąrašą eliminuojame pirmą ir paskutinį indeksą\n",
    "\n",
    "    # Naudojamų požymių sąrašas (vasariniam modeliui 1 panaikinau 'RRl/RRr')\n",
    "    all_features = ['seq_size','RR_l_0', 'RR_r_0', 'RR_r/RR_l', 'wl_side','wr_side',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "   \n",
    "    # print(\"pradėjome predict_cnn_fda_vu_v1\")\n",
    "    # nuskaitome modelio parametrus\n",
    "    model_path = Path(model_dir, 'best_model_final_2.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Nuskaitome scaler objectą\n",
    "    path_scaler = Path(model_dir, 'scaler.pkl')  \n",
    "    scaler = pickle.load(open(path_scaler,'rb'))\n",
    "\n",
    "    idx_lst = list(range(1, len(atr_sample)-1))\n",
    "\n",
    "    # Formuojame iš pūpsnių požymių masyvą\n",
    "    # data_frame, omitted = apply_FDA_vasara(signal, idx_lst, atr_sample)\n",
    "    data_frame, omitted = get_beat_features_set_fda_vu_v1_micro(signal, atr_sample, idx_lst)\n",
    "\n",
    "    data_frame = data_frame.set_index('idx')\n",
    "    data_frame.columns = data_frame.columns.astype(str)\n",
    "\n",
    "    # paruošiame požymių masyvą klasifikatoriui\n",
    "    data_frame_init = data_frame[all_features]\n",
    "    data_array = scaler.transform(data_frame_init)\n",
    "    test_x = data_array\n",
    "    x_test = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\n",
    "\n",
    "    # Pūpsnių klasių atpažinimas\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions_y = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Sužymimi neatpažinti pūpsniai - klasė 3:'U'\n",
    "    pred_y = np.zeros(len(atr_sample), dtype=int)\n",
    "    pred_y[0] = 3\n",
    "    pred_y[len(atr_sample)-1] = 3\n",
    "\n",
    "    if (omitted.empty != True):\n",
    "        idxs = list(omitted['idx'].astype('int'))\n",
    "        for i in range(len(idxs)):\n",
    "            pred_y[idxs[i]] = 3\n",
    "            \n",
    "# Sužymimi atpažinti pūpsniai \n",
    "    selected = data_frame.index.astype('int')\n",
    "    pred_y[selected] = predictions_y\n",
    "    \n",
    "    # print(f\"\\natr_sample: {atr_sample[:40]}\")\n",
    "\n",
    "    return pred_y\n",
    "\n",
    "\n",
    "# PASIRUOŠIMAS\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)  # type: ignore\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Naudojamų požymių sąrašas \n",
    "all_features = ['seq_size','RR_l_0', 'RR_r_0', 'RR_r/RR_l','wl_side','wr_side',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "\n",
    "print(\"\\nAtliekama pūpsnių pacientų įrašuose klasifikacija\")\n",
    "\n",
    "# NURODOME PACIENTŲ SĄRAŠĄ. GALIMI ĮVAIRŪS VARIANTAI\n",
    "\n",
    "# Variantas: visi duomenys\n",
    "# Nuskaitome failą info_create.json ir duomenų rinkinio parametrus\n",
    "file_path = Path(rec_dir,'info_create_z.json')\n",
    "with open(file_path) as json_file:\n",
    "    info_create = json.load(json_file)\n",
    "# SubjCodes =  info_create['SubjCodes'] # pacientų įrašų sąrašas\n",
    "\n",
    "# Variantas: mokymo imtis  \n",
    "# file_path = Path(rec_dir, 'train_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: validation imtis  \n",
    "# file_path = Path(rec_dir, 'validation_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: testinė imtis  \n",
    "file_path = Path(rec_dir, 'test_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Pacientų įrašų sąrašas testavimui\n",
    "# file_path = 'testinis_sarasas.csv'\n",
    "# SubjCodes = [10020, 10021, 10051] #Testavimui\n",
    "SubjCodes = [10022, 10021, 10083, 10091] #Testuojamas pacientų įrašų sąrašas\n",
    "\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "print(f\"Sąrašas nuskaitytas iš: {file_path}\")\n",
    "\n",
    "# Suformuojamas aplankas rezultatams\n",
    "head_tail = os.path.split(file_path)\n",
    "filename, file_extension = os.path.splitext(head_tail[1])\n",
    "path_for_results = Path(db_path, 'rezultatai' + '_' + filename)\n",
    "print(f\"\\nAplankas rezultatams: {path_for_results}\")\n",
    "create_dir(path_for_results)\n",
    "\n",
    "# Kas kiek išvedamas apdorotų sekų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Klasių simbolinių vardų sąrašas ir klasių skaičius\n",
    "class_names = list(selected_beats.keys()) \n",
    "n_classes = len(selected_beats)\n",
    "# print(class_names)\n",
    "\n",
    "# Nuskaitome pūpsnių atributų masyvą\n",
    "file_path = Path(rec_dir, all_beats_attr_fname)\n",
    "all_beats_attr = pd.read_csv(file_path, index_col=0, dtype = {'userNr': int, 'recordingNr': int,\n",
    "                                                             'sample': int, 'symbol': str, 'label': int})\n",
    "all_beat_indices = all_beats_attr.index\n",
    "\n",
    "index_start = 0\n",
    "# Sukūriame masyvą, į kurį sudėsime visų įrašų pūpsnių anotuotus ir automatiškai surastus klasių numerius\n",
    "validation_set_stats = pd.DataFrame(columns=['idx', 'test_label', 'pred_label', 'SubjCode'])\n",
    "\n",
    "start_time = time.time()\n",
    "# Ciklas per pacientų įrašus\n",
    "for SubjCode in SubjCodes:\n",
    "    \n",
    "    # Nuskaitome EKG įrašą (npy formatu)\n",
    "    sign_raw = read_rec(rec_dir, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "\n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    file_name = get_filename(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    print(f\"\\nSubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2} signal_length: {signal_length}\")\n",
    "\n",
    "    # Filtruojame signalą\n",
    "    # signal = signal_filter(signal=sign_raw, sampling_rate=200, lowcut=0.2, method=\"butterworth\", order=5)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks = read_df_rpeaks(rec_dir, SubjCode)\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "\n",
    "    # SUFORMUOJAME EKG ĮRAŠUI TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "   \n",
    "    pred_labels = predict_cnn_fda_vu_v1_micro(signal, atr_sample, model_dir)\n",
    "    # pred_labels turi būti tokio pat ilgio, kaip ir test_labels, praleisti (šiuo atveju pirmas\n",
    "    # ir paskutinis pūpsnys), o taip pat pakliuvęs į ommited sritį, pažymimi klase 3\n",
    "    if (len(test_labels) != len(pred_labels)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_labels ir pred_labels ilgiai\")     \n",
    "    \n",
    "    # print(\"test_labels:\", len(test_labels))\n",
    "    # print(test_labels)\n",
    "    # print(\"pred_labels:\", len(pred_labels))\n",
    "    # print(pred_labels)\n",
    "    \n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \",unique, counts, total)\n",
    "\n",
    "    # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':test_labels[i], 'pred':pred_labels[i]})\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    # Dirbant su daug įrašų sąrašo išvedimą reikia užblokuoti !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    classification = []  # Užblokuota\n",
    "    if (classification):\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")  \n",
    "    \n",
    "    # SUFORMUOJAME FREIMĄ validation_set_stats SU PŪPSNIŲ KLASIŲ ANOTUOTAIS IR AUTOMATIŠKAI \n",
    "    # SURASTAIS KLASIŲ NUMERIAIS, IŠMETANT KLASES SU NUMERIU = 3\n",
    "\n",
    "    # Surandame pradinį SubjCode įrašo indeksą faile all_beats_attr\n",
    "    selected_ind = all_beat_indices[(all_beats_attr['userNr']==userNr) & (all_beats_attr['recordingNr']==recNr)]\n",
    "    # print(f\"SubjCode: {SubjCode}  first elem: {selected_ind[0]} last elem: {selected_ind[-1]}  tot: {len(selected_ind)}\")\n",
    "    index_start = selected_ind[0]\n",
    "    # print('\\nSubjCode:',SubjCode, 'index_start:', index_start)   \n",
    "\n",
    "    #  Praleisdami indeksą, jei masyvuose test_labels ir pred_labels yra reikšmė == 3,\n",
    "    # suformuojame klasifikuotinų pūpsnių indeksų sąrašą\n",
    "    for idx in range(len(atr_sample)):\n",
    "        flag = (test_labels[idx] == 3) or (pred_labels[idx] == 3)\n",
    "        if (flag == False):\n",
    "            # Dėmesio: ?????????????????????????????????????????????\n",
    "            # taisytina vieta, bus problemų su pandas 1.4.1\n",
    "            validation_set_stats = validation_set_stats.append({'idx':index_start+idx, \n",
    "            'test_label':test_labels[idx],'pred_label':pred_labels[idx], 'SubjCode': SubjCode}, ignore_index=True)\n",
    "\n",
    "    # Suformuojame klasių numerių msyvus confusion matricai skaičiuoti, surandama confusion matrica\n",
    "    test_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['test_label']).astype('int') \n",
    "    # print(all_beats_attr.info())\n",
    "    pred_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['pred_label']).astype('int')\n",
    "   \n",
    "    # Atsikračius pūpsnių su klase = 3 ir suformavus masyvus, pred_y turi būti tokio pat ilgio, kaip ir test_y\n",
    "    if (len(test_y) != len(pred_y)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_y ir pred_y ilgiai\")     \n",
    "\n",
    "    confusion = confusion_matrix(test_y, pred_y)\n",
    "    # print(confusion)\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(test_y, pred_y, labels=[0, 1, 2], zero_division=0)\n",
    "\n",
    "    str1 =f\"N:{int(sup[0]):>5} S:{(int(sup[1])):3} V:{int(sup[2]):3}\" \n",
    "    str2 = f\"  Nprec:{prec[0]:>5.2f} Nrec:{rec[0]:5.2f} Nfsc:{fsc[0]:5.2f}\"\n",
    "    str3 = f\"  Sprec:{prec[1]:>5.2f} Srec:{rec[1]:5.2f} Sfsc:{fsc[1]:5.2f}\"\n",
    "    str4 = f\"  Vprec:{prec[2]:>5.2f} Vrec:{rec[2]:5.2f} Vfsc:{fsc[2]:5.2f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "    # print(len(validation_set_stats))\n",
    "    # print(len(test_y))\n",
    "    # print(len(pred_y))\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\n')\n",
    "runtime(end_time-start_time)\n",
    "\n",
    "# Sukūriame anotuotų ir automatiškai priskirtų klasių visų įrašų pūpsniams sąrašus \n",
    "validate_ind_lst = list(validation_set_stats['idx'])\n",
    "y_validate = np.array(validation_set_stats['test_label']).astype('int')\n",
    "y_predicted = np.array(validation_set_stats['pred_label']).astype('int')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODELIO TIKSLUMO VERTINIMO REZULTATAI\n",
      "Modelis iš aplanko:  model_cnn_fda_vu_v1\n",
      "Klasės ['N', 'S', 'V']: [3101   37   31] Suma: 3169 \n",
      "\n",
      "APIBENDRINTI REZULTATAI\n",
      "\n",
      "Confusion Matrix\n",
      "      N   S   V\n",
      "N  3058  39   4\n",
      "S    14   6  17\n",
      "V    14   0  17\n",
      "\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "      N     S     V\n",
      "N 0.986 0.013 0.001\n",
      "S 0.378 0.162 0.459\n",
      "V 0.452 0.000 0.548\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.991     0.986     0.989      3101\n",
      "           S      0.133     0.162     0.146        37\n",
      "           V      0.447     0.548     0.493        31\n",
      "\n",
      "    accuracy                          0.972      3169\n",
      "   macro avg      0.524     0.566     0.543      3169\n",
      "weighted avg      0.976     0.972     0.974      3169\n",
      "\n",
      "\n",
      "Apibendrinti_rezultatai įrašyti į:  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_test_subjcode_lst\\apibendrinti_rezultatai.csv\n"
     ]
    }
   ],
   "source": [
    "# MODELIO TIKSLUMO VERTINIMO IŠ VERTINIMO IMTIES REZULTATAI\n",
    "\n",
    "print(\"\\nMODELIO TIKSLUMO VERTINIMO REZULTATAI\")\n",
    "print(\"Modelis iš aplanko: \", model_dir)\n",
    "\n",
    "cols, dist, tot = get_freq_unique_values(y_validate, ['N', 'S', 'V'])\n",
    "print(f\"Klasės {cols}: {dist} Suma: {tot} \")\n",
    "\n",
    "# APIBENDRINTI REZULTATAI\n",
    "\n",
    "print('\\nAPIBENDRINTI REZULTATAI\\n')\n",
    "\n",
    "# Skaičiuojame ir išvedame klasifikavimo lentelę\n",
    "confusion = confusion_matrix(y_validate, y_predicted)\n",
    "pd.set_option('display.precision',3)\n",
    "show_confusion_matrix(confusion, class_names)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"\\nClassification Report\\n\")\n",
    "# target_names = [key for (key, value) in selected_beats.items()]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(classification_report(y_validate, y_predicted, target_names=class_names, digits=3))\n",
    "report = classification_report(y_validate, y_predicted, target_names=class_names, output_dict=True)\n",
    "# output_dictbool, default=False, If True, return output as dict.\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "# https://medium.com/@asmaiya/you-can-something-like-this-84d28e0fd31f\n",
    "\n",
    "# Įrašome į diską\n",
    "filepath = Path(path_for_results, 'apibendrinti_rezultatai.csv') \n",
    "df_report.to_csv(filepath)    \n",
    "print(f'\\nApibendrinti_rezultatai įrašyti į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLAIDŲ PASISKIRSTYMAS PER PACIENTUS IR JŲ ĮRAŠUS\n",
    "\n",
    "from zive_util_vu import zive_read_df_data, create_SubjCode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def collect_noise_locs(rec_dir, all_beats_attr):\n",
    "\n",
    "    noise_arr_tot = np.empty(0, dtype=int)\n",
    "    \n",
    "    grouped = all_beats_attr.groupby(['userNr', 'recordingNr'])\n",
    "    for key, group in grouped:\n",
    "        # print('\\n',key)\n",
    "        userNr = key[0]\n",
    "        recordingNr = key[1]\n",
    "\n",
    "        # SubjCode naudojamas atveju, kai duomenis yra formoje SubjCode.npy, SubjCode.json\n",
    "        SubjCode = create_SubjCode(userNr, recordingNr)\n",
    "        filepath = Path(rec_dir, str(SubjCode) + '.json') \n",
    "        df_noises = zive_read_df_data(filepath, 'noises')\n",
    "\n",
    "        noise_arr = np.full(shape=len(group), fill_value=0,  dtype=int)\n",
    "        idx_noise = 0\n",
    "\n",
    "        for i, row_i in group.iterrows():\n",
    "            sample = row_i['sample']\n",
    "            for j, row_j in df_noises.iterrows():\n",
    "                if ((sample > row_j['startIndex']) & (sample < row_j['endIndex'])):\n",
    "                    noise_arr[idx_noise] = 1\n",
    "                    idx_noise += 1\n",
    "        noise_arr_tot = np.append(noise_arr_tot, noise_arr)\n",
    "    return noise_arr_tot\n",
    "\n",
    "def get_error(y_test, y_pred):\n",
    "    # Error in %\n",
    "    n_errors = 0\n",
    "    for idx in range(len(y_pred)):\n",
    "        if (y_test[idx] != y_pred[idx]):\n",
    "            n_errors += 1\n",
    "    Err = float(n_errors)/len(y_pred)*100.\n",
    "    Err = round(Err, 1)\n",
    "    return Err\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Sukuriame ir užpildome dataframe su sekų parametrais\n",
    "df_seq_errors = pd.DataFrame(columns= ['idx', 'userNr', 'file_name'])\n",
    "\n",
    "rows_list = []\n",
    "for idx in validate_ind_lst:\n",
    "# Surandame  userNr, recordingNr, symbol\n",
    "    userNr, recNr, label, symbol = get_beat_attributes(idx, all_beats_attr)\n",
    "    seq_attr = {'idx': idx, 'userNr':userNr, 'recordingNr':recNr}\n",
    "    rows_list.append(seq_attr)\n",
    "\n",
    "# print(rows_list[:10])\n",
    "\n",
    "# Čia įdedame informaciją, ar pūpsnys patenka į triukšmų zoną\n",
    "noise_arr = collect_noise_locs(rec_dir, all_beats_attr)\n",
    "noise_arr = noise_arr[validate_ind_lst]\n",
    "\n",
    "# print(noise_arr)\n",
    "# unique, counts = np.unique(noise_arr, return_counts=True)\n",
    "# print(unique, counts, noise_arr.shape[0])\n",
    "# print(counts.sum())\n",
    "\n",
    "df_seq_errors = pd.DataFrame(rows_list)\n",
    "\n",
    "df_seq_errors['labels'] = pd.Series(y_validate)\n",
    "df_seq_errors['preds'] = pd.Series(y_predicted)\n",
    "zeros_arr = np.zeros( y_predicted.shape[0], dtype=int)\n",
    "df_seq_errors['errors'] = pd.Series(zeros_arr)  \n",
    "df_seq_errors.loc[df_seq_errors['labels'] != df_seq_errors['preds'], 'errors'] = 1 \n",
    "df_seq_errors['noises'] = pd.Series(noise_arr)  \n",
    "\n",
    "# print(df_seq_errors.info())\n",
    "\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientus\n",
    "\n",
    "print(\"\\nRezultatų pasiskirstymas per pacientus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_user_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'), 'userId':pd.Series(dtype='str'),  # ??????????????????????\n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "    'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Išgauname pacientų vidinę (eilės nr) numeraciją\n",
    "grouped  = df_seq_errors.groupby(['userNr'])\n",
    "userNrs = list(grouped.groups.keys())\n",
    "print(f'Pacientų: {len(userNrs)}\\n')\n",
    "\n",
    "for userNr in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[userNr]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[userNr]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "    \n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[userNr]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "    # Testavimui\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    dict_user_errors = {'userNr':int(userNr),'userId':str(userId),   # //////////////////////////////////////////\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err,  'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_user_errors)\n",
    "\n",
    "df_user_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "tit1 = f\"{'userNr':>6} {'userId':>24} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "for idx, row in  df_user_errors.iterrows():\n",
    "    str1 =f\"{int(row['userNr']):>6} {str(row['userId']):>6} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "    str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "    str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "    str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(path_for_results, 'rezultatu_pasiskirstymas_per_pacientus.csv') \n",
    "df_user_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per pacientus įrašytas į:  {filepath}')\n",
    "\n",
    "# print(df_user_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientų įrašus\n",
      "Pacientų įrašų: 4\n",
      "\n",
      "\n",
      "userNr: 1002 userId: 60e1d80f93b55b41529e9eaa\n",
      "recordingNr       file_name        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     1          1625402.027      744    4   12    0.99  1.00  0.99    0.50  0.25  0.33    0.80  0.33  0.47      1.7     56.2\n",
      "     2          1625400.796      799    2   14    0.99  1.00  1.00    0.00  0.00  0.00    0.89  0.57  0.70      1.0     52.9\n",
      "\n",
      "\n",
      "userNr: 1008 userId: 613b1d0c3d08d413ffcdc8f6\n",
      "recordingNr       file_name        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     3          1630757.924      715   11    5    0.99  1.00  1.00    0.00  0.00  0.00    0.56  1.00  0.71      1.5     94.1\n",
      "\n",
      "\n",
      "userNr: 1009 userId: 613b1d673d08d4d1f3cdc8f8\n",
      "recordingNr       file_name        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     1          1631141.764      843   20    0    1.00  0.95  0.97    0.12  0.25  0.16    0.00  0.00  0.00      6.5     41.0\n",
      "\n",
      "Rezultatų pasiskirstymas per įrašus įrašytas į:  D:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_test_subjcode_lst\\rezultatu_pasiskirstymas_per_irasus.csv\n"
     ]
    }
   ],
   "source": [
    "# Rezultatų pasiskirstymas per pacientus ir jų įrašus\n",
    "# Skaičiuojama visoms 3 klasėms Precision(tikslumas), Recall (atgaminimas), Fscore (F rodiklis)\n",
    "# \n",
    "# https://towardsdatascience.com/you-dont-always-have-to-loop-through-rows-in-pandas-22a970b347ac\n",
    "# \n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 18)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientų įrašus\n",
    "print(\"\\n\\nRezultatų pasiskirstymas per pacientų įrašus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_rec_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'),\n",
    "    'recordingNr':pd.Series(dtype='int'), 'file_name':pd.Series(dtype='str'),  \n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "     'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Sugrupuojame eilutes pagal 'userId','recordingId', suskaičiuojame, kiek kiekviename įraše\n",
    "# iš viso yra klasifikuojamų sekų (labels) ir kiek padaryta klaidų (errors).\n",
    "# Grupavimo objektą paverčiame į normalų dataframe objektą\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','recordingNr'])\n",
    "print(f'Pacientų įrašų: {grouped.ngroups}')\n",
    "# print(f'{grouped.size()=}')\n",
    "\n",
    "for key in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    # print(f'\\nGroup: {key}\\n{df_seq_errors.loc[grouped.groups[key]]}')\n",
    "    userNr = key[0]\n",
    "    recordingNr = key[1]\n",
    "    file_name = get_filename(rec_dir, create_SubjCode(userNr, recordingNr))\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[key]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[key]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "\n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[key]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "\n",
    "    # *********************** Testavimui *******************************************************************\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "    # *********************************************************************************************************\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    \n",
    "    dict_rec_errors = {'userNr':int(userNr), 'recordingNr':recordingNr, 'file_name':file_name,\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_rec_errors)\n",
    "\n",
    "df_rec_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "grouped  = df_rec_errors.groupby('userNr')\n",
    "for userNr, group in grouped:\n",
    "    # print(group.dtypes)\n",
    "    print(\"\\n\")\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "    print(f\"{'userNr:'} {userNr} {'userId:'} {userId}\" )\n",
    "    tit1 = f\"{'recordingNr':>6} {'file_name':>15} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "    tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "    tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "    tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "    print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        str1 =f\"{int(row['recordingNr']):>6} {str(row['file_name']):>20} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "        str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "        str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "        str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "        print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(path_for_results, 'rezultatu_pasiskirstymas_per_irasus.csv') \n",
    "df_rec_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per įrašus įrašytas į:  {filepath}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f36dab35816871602f0a4fffa6415a4e758bca001397bb3d9f7e90aab6637a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
