{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\n",
      "Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio\n",
      "OS in my system :  win32\n",
      "\n",
      "Bendras duomenų aplankas:  F:\\DI\\Data\\MIT&ZIVE\\VU\n",
      "Zive duomenų aplankas:  DUOM_VU\n",
      "Aplankas su originaliais EKG įrašais ir anotacijomis (.json)  F:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\records_npy\n",
      "Pūpsnių atributų failas: all_beats_attr_z.csv\n",
      "Diskretizavimo dažnis:  200\n",
      "Klasifikavimo schema: {'N': 0, 'S': 1, 'V': 2}\n",
      "Klasių skaičius: 3\n",
      "Modelio ir scaler parametrai nuskaitomas iš aplanko:  model_cnn_fda_vu_v0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Variantas, kai klasifikacija atliekama su micro moduliu, kuris klasifikuoja\n",
    "# kiekvieną pūpsnį atskirai, prieš tai apskaičiavus su kitu moduliu pūpsnio požymius.\n",
    "# Tie požymiai gali būti iš anksto įrašyti į diską.\n",
    "\n",
    "# EKG įrašų SubjCodes įmami iš test_subj_code_lst.csv,\n",
    "# suformuoto su skriptu 4_zive_creat_beats_attrib_split_v3.ipynb \n",
    "# \n",
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šis variantas pritaikytas npy formato zive įrašams, kuriems pakeistas, lyginant su \n",
    "# originaliais įrašais, failo vardas iš `file_name` į `SubjCode`, pridedant `userNr`\n",
    "# prie `file_name`. \n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos \n",
    "# paimamos iš aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys\n",
    "# iš db_folder įrašų saugyklos, jame yra ir failas all_beats_attr. \n",
    "\n",
    "# Testavimui imami įrašai iš sąrašo SubjCodes, kuris  paimamas iš mokymo, validavimo, testavimo sarašų, pvz. train_subjcode_lst.csv. Visiems įrašams iš šių\n",
    "# sąrašų egzistuoja informacija apie pūpsnius faile all_beats_attr.\n",
    " \n",
    "# Skripte yra galimybė išvesti ekstrasistolių vietas įraše.\n",
    "# Dirbant su daug įrašų reiktų užblokuoti: classification = []  # Užblokuota\n",
    " \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys, os, json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from zive_util_vu import cm2df, show_confusion_matrix \n",
    "from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "from zive_util_vu import runtime, split_SubjCode\n",
    "from zive_util_vu import get_freq_unique_values\n",
    "\n",
    "from zive_util_vu import get_beat_attributes\n",
    "from zive_util_vu import get_userId, read_rec, get_filename \n",
    "from zive_util_vu import confusion_matrix_modified, zive_read_df_rpeaks\n",
    "\n",
    "# from zive_cnn_fda_vu_v1_micro import zive_read_file_1ch\n",
    "# Pastaba: zive_read_file_1ch importuoju iš zive_cnn_fda_vu_v1, nors ji yra ir zive_util_vu.py\n",
    "# tam, kad atskirti funkcijas, kurios importuojamos skripte zive analysis, nuo tų funkcijų,\n",
    "# kurios reikalingos tik zive_accuracy_cnn_vu_v1 ir v2. \n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "print(\"Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\")\n",
    "print('Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio')\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'F:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "all_beats_attr_fname = 'all_beats_attr_z.csv'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3, 'F':3}  \n",
    "\n",
    "# Diskretizavimo dažnis:\n",
    "fs = 200\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_npy')\n",
    "\n",
    "# Nuoroda į modelio aplanką\n",
    "# model_dir = Path(Duomenu_aplankas, 'DNN', 'best_models', 'all_ft')\n",
    "model_dir = 'model_cnn_fda_vu_v0'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su originaliais EKG įrašais ir anotacijomis (.json) \", rec_dir)\n",
    "print(\"Pūpsnių atributų failas:\", all_beats_attr_fname)\n",
    "print(\"Diskretizavimo dažnis: \", fs)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "print(\"Modelio ir scaler parametrai nuskaitomas iš aplanko: \", model_dir)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulis EKG signalo pūpsnių klasifikacijai su užduotu modeliu\n",
    "\n",
    "import keras, pickle\n",
    "from get_beat_features_fda_vu import get_beat_features_fda_vu_v1\n",
    "\n",
    "def predict_cnn_fda_vu_v1_micro(signal, atr_sample, model_dir):\n",
    "# ************************************* funkcijai ***************************************************************\n",
    "\n",
    "#  Iėjimo parametrai:   signal, atr_sample, \n",
    "#                       modelio ir scaler_train parametrai\n",
    "#                       \n",
    "#  Išėjimo parametrai: pred_y, kai kurie neatpažinti : 'U':3\n",
    "\n",
    "    # Suformuojame indeksų sąrašą. Formuodami sąrašą eliminuojame pirmą ir paskutinį indeksą\n",
    "\n",
    "    all_features = ['RRl', 'RRr', 'RRl/RRr',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "   \n",
    "    print(\"pradėjome predict_cnn_fda_vu_v1\")\n",
    "    # nuskaitome modelio parametrus\n",
    "    model_path = Path(model_dir, 'best_model_final_2.h5')\n",
    "    model = keras.models.load_model(model_path)\n",
    "\n",
    "    # Nuskaitome scaler objectą\n",
    "    path_scaler = Path(model_dir, 'scaler.pkl')  \n",
    "    scaler = pickle.load(open(path_scaler,'rb'))\n",
    "\n",
    "    idx_lst = list(range(1, len(atr_sample)-1))\n",
    "    # idx_lst = [1,2]\n",
    "\n",
    "    # Formuojame iš pūpsnių požymių masyvą\n",
    "    data_frame, omitted = get_beat_features_set_fda_vu_v1(signal, atr_sample, idx_lst)\n",
    "    \n",
    "    data_frame = data_frame.set_index('idx')\n",
    "    data_frame.columns = data_frame.columns.astype(str)\n",
    "\n",
    "    # print('\\ndata_frame:')\n",
    "    # print(data_frame.head(3))\n",
    "    # print('\\nomitted:')\n",
    "    # print(omitted.head(3))\n",
    "\n",
    "    # paruošiame požymių masyvą klasifikacijai\n",
    "    data_frame_init = data_frame[all_features]\n",
    "    data_array = scaler.transform(data_frame_init)\n",
    "    test_x = data_array\n",
    "    x_test = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\n",
    "\n",
    "    # Pūpsnių klasių atpažinimas\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions_y = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Sužymimi neatpažinti pūpsniai - klasė 3:'U'\n",
    "    pred_y = np.zeros(len(atr_sample), dtype=int)\n",
    "    pred_y[0] = 3\n",
    "    pred_y[len(atr_sample)-1] = 3\n",
    "\n",
    "    if (omitted.empty != True):\n",
    "        idxs = list(omitted['idx'].astype('int'))\n",
    "        for i in range(len(idxs)):\n",
    "            pred_y[idxs[i]] = 3\n",
    "            \n",
    "# Sužymimi atpažinti pūpsniai \n",
    "    selected = data_frame.index.astype('int')\n",
    "    pred_y[selected] = predictions_y\n",
    "    \n",
    "    # print(f\"\\natr_sample: {atr_sample[:40]}\")\n",
    "\n",
    "    return pred_y\n",
    "\n",
    "# Apskaičiuojami užduotų EKG signalo pūpsnių (per idx_lst) požymiai ir iš jų\n",
    "# suformuojamas požymių dataframe masyvas\n",
    "def get_beat_features_set_fda_vu_v1(signal, atr_sample, idx_lst):\n",
    "\n",
    "    beat_features_set = pd.DataFrame()\n",
    "    omit_idx_set = pd.DataFrame()\n",
    "\n",
    "    # print(\"\\nGet beat features set from signal:\")\n",
    "    for idx in idx_lst:\n",
    "        beat_features, omit_idx = get_beat_features_fda_vu_v1(signal, atr_sample, idx)\n",
    "        # beat_features_set = beat_features_set.append(beat_features, ignore_index=True)\n",
    "        if omit_idx.empty:\n",
    "            beat_features_set = pd.concat([beat_features_set, beat_features])\n",
    "        else:\n",
    "            omit_idx_set = pd.concat(omit_idx_set, omit_idx)\n",
    "    return beat_features_set, omit_idx_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Atliekama pūpsnių pacientų įrašuose klasifikacija\n",
      "Klasifikuojamų įrašų sąrašas: [10051]\n",
      "Sąrašas nuskaitytas iš: testinis_sarasas.csv\n",
      "\n",
      "Aplankas rezultatams: F:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_testinis_sarasas\n",
      "Directory 'F:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_testinis_sarasas' created successfully\n",
      "\n",
      "SubjCode: 10051 userNr: 1005 file_name: 1630735.143 signal_length: 127999\n",
      "test_labels:  [0 1] [538   4] 542\n",
      "pradėjome predict_cnn_fda_vu_v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\base.py:329: UserWarning:Trying to unpickle estimator StandardScaler from version 1.0.1 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 6s 341ms/step\n",
      "pred_labels:  [0 2 3] [539   1   2] 542\n",
      "N:  536 S:  4 V:  0  Nprec: 0.99 Nrec: 1.00 Nfsc: 1.00  Sprec: 0.00 Srec: 0.00 Sfsc: 0.00  Vprec: 0.00 Vrec: 0.00 Vfsc: 0.00\n",
      "\n",
      "\n",
      "Runtime: 00:00:17\n"
     ]
    }
   ],
   "source": [
    "# Pagrindinis skriptas - ciklas per ECG įrašus\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"\\nAtliekama pūpsnių pacientų įrašuose klasifikacija\")\n",
    "\n",
    "# NURODOME PACIENTŲ SĄRAŠĄ. GALIMI ĮVAIRŪS VARIANTAI\n",
    "\n",
    "# Variantas: visi duomenys\n",
    "# Nuskaitome failą info_create.json ir duomenų rinkinio parametrus\n",
    "# file_path = Path(rec_dir,'info_create_z.json')\n",
    "# with open(file_path) as json_file:\n",
    "#     info_create = json.load(json_file)\n",
    "# SubjCodes =  info_create['SubjCodes'] # pacientų įrašų sąrašas\n",
    "\n",
    "# Variantas: mokymo imtis  \n",
    "# file_path = Path(rec_dir, 'train_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: validation imtis  \n",
    "# file_path = Path(rec_dir, 'validation_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: testinė imtis  \n",
    "# file_path = Path(rec_dir, 'test_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Pacientų įrašų sąrašas testavimui\n",
    "file_path = 'testinis_sarasas.csv'\n",
    "# SubjCodes = [10020, 10021, 10051] #Testavimui\n",
    "SubjCodes = [10051] #Testavimo pacientų įrašų sąrašas\n",
    "\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "print(f\"Sąrašas nuskaitytas iš: {file_path}\")\n",
    "\n",
    "# Suformuojamas aplankas rezultatams\n",
    "head_tail = os.path.split(file_path)\n",
    "filename, file_extension = os.path.splitext(head_tail[1])\n",
    "path_for_results = Path(db_path, 'rezultatai' + '_' + filename)\n",
    "print(f\"\\nAplankas rezultatams: {path_for_results}\")\n",
    "create_dir(path_for_results)\n",
    "\n",
    "# Kas kiek išvedamas apdorotų sekų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Klasių simbolinių vardų sąrašas ir klasių skaičius\n",
    "class_names = list(selected_beats.keys()) \n",
    "n_classes = len(selected_beats)\n",
    "# print(class_names)\n",
    "\n",
    "# Nuskaitome pūpsnių atributų masyvą\n",
    "file_path = Path(rec_dir, all_beats_attr_fname)\n",
    "all_beats_attr = pd.read_csv(file_path, index_col=0, dtype = {'userNr': int, 'recordingNr': int,\n",
    "                                                             'sample': int, 'symbol': str, 'label': int})\n",
    "all_beat_indices = all_beats_attr.index\n",
    "index_start = 0\n",
    "\n",
    "# Sukūriame masyvą, į kurį sudėsime visų įrašų pūpsnių anotuotus ir automatiškai surastus klasių numerius\n",
    "validation_set_stats = pd.DataFrame(columns=['idx', 'test_label', 'pred_label', 'SubjCode'])\n",
    "\n",
    "start_time = time.time()\n",
    "# Ciklas per pacientų įrašus\n",
    "for SubjCode in SubjCodes:\n",
    "    \n",
    "    # Nuskaitome EKG įrašą (npy formatu)\n",
    "    sign_raw = read_rec(rec_dir, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "\n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    file_name = get_filename(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    print(f\"\\nSubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2} signal_length: {signal_length}\")\n",
    "\n",
    "    # Filtruojame signalą\n",
    "    # signal = signal_filter(signal=sign_raw, sampling_rate=200, lowcut=0.2, method=\"butterworth\", order=5)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    df_rpeaks = zive_read_df_rpeaks(rec_dir, str(SubjCode))\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "\n",
    "    # SUFORMUOJAME EKG ĮRAŠUI TESTINĮ ir PRISKIRTŲ KLASIŲ NUMERIŲ MASYVUS\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "\n",
    "    (unique, counts) = np.unique(test_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"test_labels: \", unique, counts, total)\n",
    "   \n",
    "    # EKG signalo pūpsnių klasifikacija su užduotu modeliu iš model_dir\n",
    "    pred_labels = predict_cnn_fda_vu_v1_micro(signal, atr_sample, model_dir)\n",
    "\n",
    "    # pred_labels turi būti tokio pat ilgio, kaip ir test_labels, praleisti (šiuo atveju pirmas\n",
    "    # ir paskutinis pūpsnys), o taip pat pakliuvęs į ommited sritį, pažymimi klase 3\n",
    "    if (len(test_labels) != len(pred_labels)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_labels ir pred_labels ilgiai\")     \n",
    "\n",
    "    (unique, counts) = np.unique(pred_labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    print(\"pred_labels: \",unique, counts, total)\n",
    "\n",
    "    # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':test_labels[i], 'pred':pred_labels[i]})\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    # Dirbant su daug įrašų sąrašo išvedimą reikia užblokuoti !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    classification = []  # Užblokuota\n",
    "    if (classification):\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")  \n",
    "    \n",
    "    # SUFORMUOJAME FREIMĄ validation_set_stats SU PŪPSNIŲ KLASIŲ ANOTUOTAIS IR AUTOMATIŠKAI \n",
    "    # SURASTAIS KLASIŲ NUMERIAIS, IŠMETANT KLASES SU NUMERIU = 3\n",
    "\n",
    "    # Surandame pradinį SubjCode įrašo indeksą faile all_beats_attr\n",
    "    selected_ind = all_beat_indices[(all_beats_attr['userNr']==userNr) & (all_beats_attr['recordingNr']==recNr)]\n",
    "    # print(f\"SubjCode: {SubjCode}  first elem: {selected_ind[0]} last elem: {selected_ind[-1]}  tot: {len(selected_ind)}\")\n",
    "    index_start = selected_ind[0]\n",
    "    # print('\\nSubjCode:',SubjCode, 'index_start:', index_start)   \n",
    "\n",
    "    #  Praleisdami indeksą, jei masyvuose test_labels ir pred_labels yra reikšmė == 3,\n",
    "    # suformuojame klasifikuotinų pūpsnių indeksų sąrašą\n",
    "    for idx in range(len(atr_sample)):\n",
    "        flag = (test_labels[idx] == 3) or (pred_labels[idx] == 3)\n",
    "        if (flag == False):\n",
    "            validation_stats = pd.DataFrame([{'idx':index_start+idx,'test_label':test_labels[idx],\n",
    "                                                'pred_label':pred_labels[idx], 'SubjCode': SubjCode}])\n",
    "            validation_set_stats = pd.concat([validation_set_stats, validation_stats])\n",
    "\n",
    "    # Suformuojame klasių numerių msyvus confusion matricai skaičiuoti, surandama confusion matrica\n",
    "    test_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['test_label']).astype('int') \n",
    "    # print(all_beats_attr.info())\n",
    "    pred_y = np.array(validation_set_stats[validation_set_stats['SubjCode']==SubjCode]['pred_label']).astype('int')\n",
    "   \n",
    "    # Atsikračius pūpsnių su klase = 3 ir suformavus masyvus, pred_y turi būti tokio pat ilgio, kaip ir test_y\n",
    "    if (len(test_y) != len(pred_y)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_y ir pred_y ilgiai\")     \n",
    "\n",
    "    confusion = confusion_matrix(test_y, pred_y)\n",
    "    # print(confusion)\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(test_y, pred_y, labels=[0, 1, 2], zero_division=0)\n",
    "\n",
    "    str1 =f\"N:{int(sup[0]):>5} S:{(int(sup[1])):3} V:{int(sup[2]):3}\" \n",
    "    str2 = f\"  Nprec:{prec[0]:>5.2f} Nrec:{rec[0]:5.2f} Nfsc:{fsc[0]:5.2f}\"\n",
    "    str3 = f\"  Sprec:{prec[1]:>5.2f} Srec:{rec[1]:5.2f} Sfsc:{fsc[1]:5.2f}\"\n",
    "    str4 = f\"  Vprec:{prec[2]:>5.2f} Vrec:{rec[2]:5.2f} Vfsc:{fsc[2]:5.2f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "    # print(len(validation_set_stats))\n",
    "    # print(len(test_y))\n",
    "    # print(len(pred_y))\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\n')\n",
    "runtime(end_time-start_time)\n",
    "\n",
    "# Sukūriame anotuotų ir automatiškai priskirtų klasių visų įrašų pūpsniams sąrašus \n",
    "validate_ind_lst = list(validation_set_stats['idx'])\n",
    "y_validate = np.array(validation_set_stats['test_label']).astype('int')\n",
    "y_predicted = np.array(validation_set_stats['pred_label']).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODELIO TIKSLUMO VERTINIMO REZULTATAI\n",
      "Modelis iš aplanko:  model_cnn_fda_vu_v0\n",
      "Klasės ['N', 'S', 'V']: [536   4] Suma: 540 \n",
      "\n",
      "APIBENDRINTI REZULTATAI\n",
      "\n",
      "Confusion Matrix\n",
      "     N  S  V\n",
      "N  535  0  1\n",
      "S    4  0  0\n",
      "V    0  0  0\n",
      "\n",
      "\n",
      "Zero values! Cannot calculate Normalized Confusion Matrix\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.993     0.998     0.995       536\n",
      "           S      0.000     0.000     0.000         4\n",
      "           V      0.000     0.000     0.000         0\n",
      "\n",
      "    accuracy                          0.991       540\n",
      "   macro avg      0.331     0.333     0.332       540\n",
      "weighted avg      0.985     0.991     0.988       540\n",
      "\n",
      "\n",
      "Apibendrinti_rezultatai įrašyti į:  F:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_testinis_sarasas\\apibendrinti_rezultatai.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " f:\\DI\\ZIVEO_2022_RUDUO\\VU_APRIL_tesinys\\zive_util_vu.py:456: FutureWarning:The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      " f:\\DI\\ZIVEO_2022_RUDUO\\VU_APRIL_tesinys\\zive_util_vu.py:456: FutureWarning:The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      " f:\\DI\\ZIVEO_2022_RUDUO\\VU_APRIL_tesinys\\zive_util_vu.py:456: FutureWarning:The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      " c:\\Users\\kesju\\miniconda3\\envs\\mlflow_test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning:Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "# MODELIO TIKSLUMO VERTINIMO IŠ VERTINIMO IMTIES REZULTATAI\n",
    "\n",
    "print(\"\\nMODELIO TIKSLUMO VERTINIMO REZULTATAI\")\n",
    "print(\"Modelis iš aplanko: \", model_dir)\n",
    "\n",
    "cols, dist, tot = get_freq_unique_values(y_validate, ['N', 'S', 'V'])\n",
    "print(f\"Klasės {cols}: {dist} Suma: {tot} \")\n",
    "\n",
    "# APIBENDRINTI REZULTATAI\n",
    "\n",
    "print('\\nAPIBENDRINTI REZULTATAI\\n')\n",
    "\n",
    "# Skaičiuojame ir išvedame klasifikavimo lentelę\n",
    "confusion = confusion_matrix(y_validate, y_predicted)\n",
    "pd.set_option('display.precision',3)\n",
    "show_confusion_matrix(confusion, class_names)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"\\nClassification Report\\n\")\n",
    "# target_names = [key for (key, value) in selected_beats.items()]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(classification_report(y_validate, y_predicted, target_names=class_names, digits=3))\n",
    "report = classification_report(y_validate, y_predicted, target_names=class_names, output_dict=True)\n",
    "# output_dictbool, default=False, If True, return output as dict.\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "# https://medium.com/@asmaiya/you-can-something-like-this-84d28e0fd31f\n",
    "\n",
    "# Įrašome į diską\n",
    "filepath = Path(path_for_results, 'apibendrinti_rezultatai.csv') \n",
    "df_report.to_csv(filepath)    \n",
    "print(f'\\nApibendrinti_rezultatai įrašyti į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultatų pasiskirstymas per pacientus\n",
      "Pacientų: 1\n",
      "\n",
      "userNr                   userId        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "  1005 613b1c6f3d08d4370acdc8f3      536    4    0    0.99  1.00  1.00    0.00  0.00  0.00    0.00  0.00  0.00      0.9      0.0\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientus įrašytas į:  F:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_testinis_sarasas\\rezultatu_pasiskirstymas_per_pacientus.csv\n"
     ]
    }
   ],
   "source": [
    "# KLAIDŲ PASISKIRSTYMAS PER PACIENTUS IR JŲ ĮRAŠUS\n",
    "\n",
    "from zive_util_vu import zive_read_df_data, create_SubjCode\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def collect_noise_locs(rec_dir, all_beats_attr):\n",
    "\n",
    "    noise_arr_tot = np.empty(0, dtype=int)\n",
    "    \n",
    "    grouped = all_beats_attr.groupby(['userNr', 'recordingNr'])\n",
    "    for key, group in grouped:\n",
    "        # print('\\n',key)\n",
    "        userNr = key[0]\n",
    "        recordingNr = key[1]\n",
    "\n",
    "        # SubjCode naudojamas atveju, kai duomenis yra formoje SubjCode.npy, SubjCode.json\n",
    "        SubjCode = create_SubjCode(userNr, recordingNr)\n",
    "        filepath = Path(rec_dir, str(SubjCode) + '.json') \n",
    "        df_noises = zive_read_df_data(filepath, 'noises')\n",
    "\n",
    "        noise_arr = np.full(shape=len(group), fill_value=0,  dtype=int)\n",
    "        idx_noise = 0\n",
    "\n",
    "        for i, row_i in group.iterrows():\n",
    "            sample = row_i['sample']\n",
    "            for j, row_j in df_noises.iterrows():\n",
    "                if ((sample > row_j['startIndex']) & (sample < row_j['endIndex'])):\n",
    "                    noise_arr[idx_noise] = 1\n",
    "                    idx_noise += 1\n",
    "        noise_arr_tot = np.append(noise_arr_tot, noise_arr)\n",
    "    return noise_arr_tot\n",
    "\n",
    "def get_error(y_test, y_pred):\n",
    "    # Error in %\n",
    "    n_errors = 0\n",
    "    for idx in range(len(y_pred)):\n",
    "        if (y_test[idx] != y_pred[idx]):\n",
    "            n_errors += 1\n",
    "    Err = float(n_errors)/len(y_pred)*100.\n",
    "    Err = round(Err, 1)\n",
    "    return Err\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Sukuriame ir užpildome dataframe su sekų parametrais\n",
    "df_seq_errors = pd.DataFrame(columns= ['idx', 'userNr', 'file_name'])\n",
    "\n",
    "rows_list = []\n",
    "for idx in validate_ind_lst:\n",
    "# Surandame  userNr, recordingNr, symbol\n",
    "    userNr, recNr, label, symbol = get_beat_attributes(idx, all_beats_attr)\n",
    "    seq_attr = {'idx': idx, 'userNr':userNr, 'recordingNr':recNr}\n",
    "    rows_list.append(seq_attr)\n",
    "\n",
    "# print(rows_list[:10])\n",
    "\n",
    "# Čia įdedame informaciją, ar pūpsnys patenka į triukšmų zoną\n",
    "noise_arr = collect_noise_locs(rec_dir, all_beats_attr)\n",
    "noise_arr = noise_arr[validate_ind_lst]\n",
    "\n",
    "# print(noise_arr)\n",
    "# unique, counts = np.unique(noise_arr, return_counts=True)\n",
    "# print(unique, counts, noise_arr.shape[0])\n",
    "# print(counts.sum())\n",
    "\n",
    "df_seq_errors = pd.DataFrame(rows_list)\n",
    "\n",
    "df_seq_errors['labels'] = pd.Series(y_validate)\n",
    "df_seq_errors['preds'] = pd.Series(y_predicted)\n",
    "zeros_arr = np.zeros( y_predicted.shape[0], dtype=int)\n",
    "df_seq_errors['errors'] = pd.Series(zeros_arr)  \n",
    "df_seq_errors.loc[df_seq_errors['labels'] != df_seq_errors['preds'], 'errors'] = 1 \n",
    "df_seq_errors['noises'] = pd.Series(noise_arr)  \n",
    "\n",
    "# print(df_seq_errors.info())\n",
    "\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientus\n",
    "\n",
    "print(\"\\nRezultatų pasiskirstymas per pacientus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_user_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'), 'userId':pd.Series(dtype='str'),  # ??????????????????????\n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "    'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Išgauname pacientų vidinę (eilės nr) numeraciją\n",
    "grouped  = df_seq_errors.groupby(['userNr'])\n",
    "userNrs = list(grouped.groups.keys())\n",
    "print(f'Pacientų: {len(userNrs)}\\n')\n",
    "\n",
    "for userNr in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[userNr]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[userNr]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "    \n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[userNr]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "    # Testavimui\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    dict_user_errors = {'userNr':int(userNr),'userId':str(userId),   # //////////////////////////////////////////\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err,  'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_user_errors)\n",
    "\n",
    "df_user_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "tit1 = f\"{'userNr':>6} {'userId':>24} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "for idx, row in  df_user_errors.iterrows():\n",
    "    str1 =f\"{int(row['userNr']):>6} {str(row['userId']):>6} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "    str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "    str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "    str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "    print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(path_for_results, 'rezultatu_pasiskirstymas_per_pacientus.csv') \n",
    "df_user_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per pacientus įrašytas į:  {filepath}')\n",
    "\n",
    "# print(df_user_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientų įrašus\n",
      "Pacientų įrašų: 1\n",
      "\n",
      "\n",
      "userNr: 1005 userId: 613b1c6f3d08d4370acdc8f3\n",
      "recordingNr       file_name        N    S    V   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     1          1630735.143      536    4    0    0.99  1.00  1.00    0.00  0.00  0.00    0.00  0.00  0.00      0.9      0.0\n",
      "\n",
      "Rezultatų pasiskirstymas per įrašus įrašytas į:  F:\\DI\\Data\\MIT&ZIVE\\VU\\DUOM_VU\\rezultatai_testinis_sarasas\\rezultatu_pasiskirstymas_per_irasus.csv\n"
     ]
    }
   ],
   "source": [
    "# Rezultatų pasiskirstymas per pacientus ir jų įrašus\n",
    "# Skaičiuojama visoms 3 klasėms Precision(tikslumas), Recall (atgaminimas), Fscore (F rodiklis)\n",
    "# \n",
    "# https://towardsdatascience.com/you-dont-always-have-to-loop-through-rows-in-pandas-22a970b347ac\n",
    "# \n",
    "pd.set_option(\"display.max_rows\", 6000, \"display.max_columns\", 18)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientų įrašus\n",
    "print(\"\\n\\nRezultatų pasiskirstymas per pacientų įrašus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_rec_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'),\n",
    "    'recordingNr':pd.Series(dtype='int'), 'file_name':pd.Series(dtype='str'),  \n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "     'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Sugrupuojame eilutes pagal 'userId','recordingId', suskaičiuojame, kiek kiekviename įraše\n",
    "# iš viso yra klasifikuojamų sekų (labels) ir kiek padaryta klaidų (errors).\n",
    "# Grupavimo objektą paverčiame į normalų dataframe objektą\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','recordingNr'])\n",
    "print(f'Pacientų įrašų: {grouped.ngroups}')\n",
    "# print(f'{grouped.size()=}')\n",
    "\n",
    "for key in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    # print(f'\\nGroup: {key}\\n{df_seq_errors.loc[grouped.groups[key]]}')\n",
    "    userNr = key[0]\n",
    "    recordingNr = key[1]\n",
    "    file_name = get_filename(rec_dir, create_SubjCode(userNr, recordingNr))\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[key]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[key]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "\n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[key]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "\n",
    "    # *********************** Testavimui *******************************************************************\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "    # *********************************************************************************************************\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test, y_pred, labels=[0, 1, 2], zero_division=0)\n",
    "    \n",
    "    dict_rec_errors = {'userNr':int(userNr), 'recordingNr':recordingNr, 'file_name':file_name,\n",
    "    'N':sup[0], 'S':sup[1], 'V':sup[2],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_rec_errors)\n",
    "\n",
    "df_rec_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "grouped  = df_rec_errors.groupby('userNr')\n",
    "for userNr, group in grouped:\n",
    "    # print(group.dtypes)\n",
    "    print(\"\\n\")\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "    print(f\"{'userNr:'} {userNr} {'userId:'} {userId}\" )\n",
    "    tit1 = f\"{'recordingNr':>6} {'file_name':>15} {'N':>8} {'S':>4} {'V':>4}\"\n",
    "    tit2 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "    tit3 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "    tit4 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "    print(tit1+tit2+tit3+tit4)\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        str1 =f\"{int(row['recordingNr']):>6} {str(row['file_name']):>20} {int(row['N']):>8} {int(row['S']):4} {int(row['V']):4}\" \n",
    "        str2 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "        str3 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "        str4 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "        print(str1+str2+str3+str4)\n",
    "\n",
    "filepath = Path(path_for_results, 'rezultatu_pasiskirstymas_per_irasus.csv') \n",
    "df_rec_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per įrašus įrašytas į:  {filepath}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('mlflow_test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d054124e4d11bf3212b6a9b6947a3d0c2469a0d5e4f1a16cbe902816f811b17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
