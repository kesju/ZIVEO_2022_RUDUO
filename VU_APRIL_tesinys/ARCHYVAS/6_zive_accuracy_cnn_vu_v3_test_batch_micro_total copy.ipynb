{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.2\n"
     ]
    }
   ],
   "source": [
    "# # Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui.\n",
    "# Taisomas pritaikyti ir mit2zive duomenims\n",
    "# \n",
    "# Skriptas pritaikytas tik Zive duomenims, duomenims mit2zive duomenims skriptas nepritaikytas\n",
    "# \n",
    "# Taisoma - viskas pataisyta:\n",
    "# - pakeisti ML anotacijų pavadinimus N į Npred, S į Spred, V į Vpred, pridėti Upred\n",
    "# - pridėti N, S, V, U - čia bus tikrieji\n",
    "# - pridėti eilutės pabaigoje recordingId, userID\n",
    "# - pakeisti anotacijų pasiskirstyme anotacijų skaičius į simbolius N, S, V, U\n",
    "# - anotuoti U klasifikuojant ir skaičiuojant klaidas turi būti praleisti\n",
    "\n",
    "# Variantas v3 - skirtas atsikratyti iš anksto parengtam failui all_beats_attr. Bandoma jį suformuoti eigoje\n",
    "# Perdarytas iš 6_zive_accuracy_cnn_vu_v2_test_batch_micro.ipynb\n",
    "# Skrtas masiniam testavimui - variantas su model_cnn_fda_vu_v1\n",
    "\n",
    "# Perdarytas iš 6_zive_accuracy_cnn_vu_v2_test_batch.ipynb:\n",
    "# - pritaikytas variantui, kai klasifikacija izoliuota vienam pūpsniui\n",
    "# - pakeistas naudojamų požymių sąrašas (pritaikytas vasariniam modeliui 1)\n",
    "# - naudoja požymių skaičiavimą iš zive_cnn_fda_vu_v3_micro.py (from zive_cnn_fda_vu_v3_micro import predict_cnn_fda_vu_v1),\n",
    "# kurį gavau iš Povilo 2022 10 05 - faile ReadSeqEKG_2022_10_05.py: get_spike_width funkcija keičiasi,\n",
    "#  keičiasi ir apply_FDA (perpavardinau į get_beat_features_set_fda_vu_v1)\n",
    "# \n",
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "# Šis variantas pritaikytas npy formato zive įrašams, kuriems pakeistas, lyginant su \n",
    "# originaliais įrašais, failo vardas iš `file_name` į `SubjCode`, pridedant `userNr`\n",
    "# prie `file_name`. \n",
    "#\n",
    "# Skriptas zive EKG pūpsnių CNN VU klasifikatoriaus testavimui ir tikslumo įvertinimui, funkcijos \n",
    "# paimamos iš aplanko zive_cnn_fda_vu_v1.py, modelis iš model_cnn_fda_vu_v1, testuojami duomenys\n",
    "# iš db_folder įrašų saugyklos, jame yra ir failas all_beats_attr. \n",
    "\n",
    "# Testavimui imami įrašai iš sąrašo SubjCodes, kuris arba paimamas if failo info_create.json,\n",
    "# arba iš mokymo, validavimo, testavimo sarašų, pvz. train_subjcode_lst.csv. Visiems įrašams iš šių\n",
    "# sąrašų egzistuoja informacija apie pūpsnius faile all_beats_attr.\n",
    " \n",
    "# Skripte yra galimybė išvesti ekstrasistolių vietas įraše.\n",
    "# Dirbant su daug įrašų reiktų užblokuoti: classification = []  # Užblokuota\n",
    " \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "# import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys, os, json\n",
    "from pathlib import Path\n",
    "# from icecream import ic\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from zive_util_vu import confusion_matrix_modified\n",
    "from zive_util_vu import cm2df, show_confusion_matrix\n",
    "\n",
    "from zive_util_vu import create_dir, create_subdir, get_rev_dictionary\n",
    "from zive_util_vu import runtime, split_SubjCode, get_SubjCode, create_SubjCode\n",
    "\n",
    "from zive_util_vu import get_beat_attributes, read_rec_attrib\n",
    "from zive_util_vu import get_userId, read_rec, read_seq\n",
    "from zive_util_vu import read_rec, read_seq\n",
    "from zive_util_vu import get_recId, get_symbol_list\n",
    "\n",
    "from zive_util_vu import zive_read_df_data, get_rec_file_name, get_label_sums, get_rid_off_class_3\n",
    "\n",
    "from zive_cnn_fda_vu_v3_micro import get_beat_features_fda_vu_v1_vasara\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "# Pastaba: zive_read_file_1ch importuoju iš zive_cnn_fda_vu_v1, nors ji yra ir zive_util_vu.py\n",
    "# tam, kad atskirti funkcijas, kurios importuojamos skripte zive analysis, nuo tų funkcijų,\n",
    "# kurios reikalingos tik zive_accuracy_cnn_vu_v1 ir v2. \n",
    "\n",
    "def get_beat_features_set_fda_vu_v1_micro(signal, atr_sample, idx_lst):\n",
    "# Apskaičiuojami užduotų EKG signalo pūpsnių (per idx_lst) požymiai ir iš jų\n",
    "# suformuojamas požymių dataframe masyvas, pridedant 'label'\n",
    "\n",
    "    #  all_beats pritaikytas MIT2ZIVE ir ZIVE duomenims\n",
    "    all_beats = {'N':0,'R':0, 'L':0, 'e':0, 'j':0, 'A':1,'a':1, 'J':1, 'S':1, 'V':2, 'E':2, 'F':3, 'U':3, 'Q':3}\n",
    "\n",
    "    beat_features_set = pd.DataFrame()\n",
    "    omit_idx_set = pd.DataFrame()\n",
    "\n",
    "    # atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    # atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    # test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "    \n",
    "    # print(\"\\nGet beat features set from signal:\")\n",
    "    for idx in idx_lst:\n",
    "        beat_features, omit_idx = get_beat_features_fda_vu_v1_vasara(signal, atr_sample, idx)\n",
    "        # beat_features_set = beat_features_set.append(beat_features, ignore_index=True)\n",
    "        if omit_idx.empty:\n",
    "            # beat_features['label'] = test_labels[idx]\n",
    "            beat_features_set = pd.concat([beat_features_set, beat_features])\n",
    "        else:\n",
    "            omit_idx_set = pd.concat([omit_idx_set, omit_idx])\n",
    "\n",
    "    # Konvertuojame int pozymius į float64\n",
    "    beat_features_set['RR_l_0'] = beat_features_set['RR_l_0'].astype(float)\n",
    "    beat_features_set['RR_r_0'] = beat_features_set['RR_r_0'].astype(float)\n",
    "\n",
    "    return beat_features_set, omit_idx_set\n",
    "\n",
    "\n",
    "def predict_cnn_fda_vu_v1_micro(signal, atr_sample, atr_symbol, model_dir):\n",
    "# ************************************* funkcijai ***************************************************************\n",
    "\n",
    "#  Iėjimo parametrai:   signal, atr_sample, \n",
    "#                       modelio ir scaler_train parametrai\n",
    "#                       \n",
    "#  Išėjimo parametrai: pred_y, kai kurie neatpažinti : 'U':3\n",
    "\n",
    "    # Suformuojame indeksų sąrašą. Formuodami sąrašą eliminuojame pirmą ir paskutinį indeksą\n",
    "\n",
    "    # Naudojamų požymių sąrašas (vasariniam modeliui 1 panaikinau 'RRl/RRr')\n",
    "    all_features = ['seq_size','RR_l_0', 'RR_r_0', 'RR_r/RR_l', 'wl_side','wr_side',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "   \n",
    "    # print(\"pradėjome predict_cnn_fda_vu_v1\")\n",
    "    # nuskaitome modelio parametrus\n",
    "    model_path = Path(model_dir, 'best_model_final_2.h5')\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # Nuskaitome scaler objectą\n",
    "    path_scaler = Path(model_dir, 'scaler.pkl')  \n",
    "    scaler = pickle.load(open(path_scaler,'rb'))\n",
    "\n",
    "    idx_lst = list(range(1, len(atr_sample)-1))\n",
    "\n",
    "    # Formuojame iš pūpsnių požymių masyvą\n",
    "    # data_frame, omitted = apply_FDA_vasara(signal, idx_lst, atr_sample)\n",
    "    data_frame, omitted = get_beat_features_set_fda_vu_v1_micro(signal, atr_sample, idx_lst)\n",
    "\n",
    "    data_frame = data_frame.set_index('idx')\n",
    "    data_frame.columns = data_frame.columns.astype(str)\n",
    "\n",
    "    # paruošiame požymių masyvą klasifikatoriui\n",
    "    data_frame_init = data_frame[all_features]\n",
    "    data_array = scaler.transform(data_frame_init)\n",
    "    test_x = data_array\n",
    "    x_test = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\n",
    "\n",
    "    # Pūpsnių klasių atpažinimas\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions_y = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Sužymimi neatpažinti pūpsniai - klasė 3:'U'\n",
    "    pred_y = np.zeros(len(atr_sample), dtype=int)\n",
    "    pred_y[0] = 3\n",
    "    pred_y[len(atr_sample)-1] = 3\n",
    "\n",
    "    if (omitted.empty != True):\n",
    "        idxs = list(omitted['idx'].astype('int'))\n",
    "        for i in range(len(idxs)):\n",
    "            pred_y[idxs[i]] = 3\n",
    "\n",
    "# Sužymimi atpažinti pūpsniai \n",
    "    selected = data_frame.index.astype('int')\n",
    "    pred_y[selected] = predictions_y\n",
    "\n",
    "# Atpažinti pūpsniai, kurie anotuoti simboliu `U`, peržymimi į klasę '3'\n",
    "# https://www.geeksforgeeks.org/python-get-the-indices-of-all-occurrences-of-an-element-in-a-list/\n",
    "    indices = np.where(atr_symbol == 'U')[0]\n",
    "    flag = np.size(indices)\n",
    "    if (flag):\n",
    "        pred_y[indices] = 3\n",
    "\n",
    "    indices = np.where(atr_symbol == 'F')[0]\n",
    "    flag = np.size(indices)\n",
    "    if (flag):\n",
    "        pred_y[indices] = 3\n",
    "\n",
    "    return pred_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\n",
      "Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio\n",
      "OS in my system :  win32\n",
      "\n",
      "Bendras duomenų aplankas:  D:\\DI\n",
      "Zive duomenų aplankas:  DUOM_2022_RUDUO\n",
      "Aplankas su originaliais EKG įrašais ir anotacijomis (.json)  D:\\DI\\DUOM_2022_RUDUO\\records_npy\n",
      "Diskretizavimo dažnis:  200\n",
      "Klasifikavimo schema: {'N': 0, 'S': 1, 'V': 2}\n",
      "Klasių skaičius: 3\n",
      "Visos galimos anotacijos: ['N', 'S', 'V', 'U']\n",
      "Modelio ir scaler parametrai nuskaitomas iš aplanko:  model_cnn_fda_vu_v1\n",
      "Atliekama pūpsnių su anotacijomis ['N', 'S', 'V'] pacientų įrašuose klasifikacija\n",
      "Klasifikuojamų įrašų sąrašas: [105, 10190]\n",
      "Sąrašas nuskaitytas iš: D:\\DI\\DUOM_2022_RUDUO\\records_npy\\info_create_z.json\n",
      "\n",
      "Aplankas rezultatams: D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z\n",
      "Directory 'D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z' already exists\n",
      "\n",
      "SubjCode: 105 userNr: 105 file_name: 105.npy userId: 105 recId: 0 signal_length: 361111\n",
      "N: 2526 S:  0 V: 41 U:124  Nml: 2476 Sml:  6 Vml: 49 Uml:160  Nprec: 1.00 Nrec: 0.99 Nfsc: 1.00  Sprec: 0.00 Srec: 0.00 Sfsc: 0.00  Vprec: 0.82 Vrec: 1.00 Vfsc: 0.90\n",
      "\n",
      "SubjCode: 10190 userNr: 1019 file_name: 1631036.053 userId: 6144c682bd0cc5acb7275368 recId: 6145fd97bd0cc5ed18275bc8 signal_length: 127999\n",
      "N:  875 S:  0 V:123 U:  0  Nml:  876 Sml:  1 Vml:119 Uml:  2  Nprec: 1.00 Nrec: 1.00 Nfsc: 1.00  Sprec: 0.00 Srec: 0.00 Sfsc: 0.00  Vprec: 0.99 Vrec: 0.96 Vfsc: 0.98\n",
      "\n",
      "\n",
      "Runtime: 00:02:44\n",
      "\n",
      "Rezultatai įrašyti į:  D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z\\klasifikacijos_rezultatai_irasams.csv\n",
      "\n",
      "Atributų freimas įrašytas: į  D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z\\all_beats_attr.csv \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pagrindinis skriptas\n",
    "\n",
    "print(\"Skriptas zive-arrh EKG segmentų apmokyto klasifikatoriaus tikslumo įvertinimui\")\n",
    "print('Modelis CNN VU su EKG sekos reikšmėmis, EKG formos požymiais, RR intervalais prieš ir po R dantelio')\n",
    "\n",
    "my_os=sys.platform\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\\\DI'   # variantas: Windows\n",
    "    # Duomenu_aplankas = 'F:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Herkulis\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "# Vietinės talpyklos aplankas ir pūpsnių atributų failas\n",
    "db_folder = 'DUOM_2022_RUDUO'\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "selected_beats = {'N':0, 'S':1, 'V':2}\n",
    "all_beats =  {'N':0, 'S':1, 'V':2, 'U':3}  \n",
    "\n",
    "# Diskretizavimo dažnis:\n",
    "fs = 200\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "#  Nuoroda į aplanką su MIT2ZIVE duomenų rinkiniu\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, 'records_npy')\n",
    "\n",
    "# Nuoroda į modelio aplanką\n",
    "# model_dir = Path(Duomenu_aplankas, 'DNN', 'best_models', 'all_ft')\n",
    "model_dir = 'model_cnn_fda_vu_v1'\n",
    "\n",
    "# Išvedame parametrus\n",
    "print(\"\\nBendras duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive duomenų aplankas: \", db_folder)\n",
    "print(\"Aplankas su originaliais EKG įrašais ir anotacijomis (.json) \", rec_dir)\n",
    "print(\"Diskretizavimo dažnis: \", fs)\n",
    "print('Klasifikavimo schema:', selected_beats)\n",
    "print('Klasių skaičius:', len(selected_beats))\n",
    "print('Visos galimos anotacijos:', list(all_beats.keys()))\n",
    "print(\"Modelio ir scaler parametrai nuskaitomas iš aplanko: \", model_dir)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# PASIRUOŠIMAS\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000)\n",
    "pd.set_option(\"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Naudojamų požymių sąrašas \n",
    "all_features = ['seq_size','RR_l_0', 'RR_r_0', 'RR_r/RR_l','wl_side','wr_side',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "                '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32',\n",
    "                '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46',\n",
    "                '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\n",
    "                '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74',\n",
    "                '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
    "                '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102',\n",
    "                '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114',\n",
    "                '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126',\n",
    "                '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138',\n",
    "                '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150',\n",
    "                '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162',\n",
    "                '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174',\n",
    "                '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186',\n",
    "                '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198',\n",
    "                '199']\n",
    "\n",
    "print(f\"Atliekama pūpsnių su anotacijomis {list(selected_beats.keys())} pacientų įrašuose klasifikacija\")\n",
    "\n",
    "# NURODOME PACIENTŲ SĄRAŠĄ. GALIMI ĮVAIRŪS VARIANTAI\n",
    "\n",
    "# Variantas: visi duomenys\n",
    "# Nuskaitome failą info_create.json ir duomenų rinkinio parametrus\n",
    "file_path = Path(rec_dir,'info_create_z.json')\n",
    "with open(file_path) as json_file:\n",
    "    info_create = json.load(json_file)\n",
    "# SubjCodes =  info_create['SubjCodes'] # pacientų įrašų sąrašas\n",
    "\n",
    "# Variantas: mokymo imtis  \n",
    "# file_path = Path(rec_dir, 'train_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: validation imtis  \n",
    "# file_path = Path(rec_dir, 'validation_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Variantas: testinė imtis  \n",
    "# file_path = Path(rec_dir, 'test_subjcode_lst.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "# Pacientų įrašų sąrašas testavimui\n",
    "# file_path = 'testinis_sarasas.csv'\n",
    "# SubjCodes = [10020, 10021, 10051] #Testavimui\n",
    "# SubjCodes = [10022, 10021, 10083, 10091] #Testuojamas pacientų įrašų sąrašas\n",
    "# SubjCodes = [105411, 10190,10022,10021, 10083, 100, 10091, 10092, 105] #Testavimui\n",
    "# SubjCodes = [10091, 10092] #Testavimui\n",
    "# SubjCodes = [10091] #Testavimui\n",
    "SubjCodes = [105, 10190] #Testavimui\n",
    "# SubjCodes = [10091,10092,300] #Testavimui\n",
    "# SubjCodes = [10091,100] #Testavimui\n",
    "# Variantas: testinė imtis \n",
    "\n",
    "# mark1\n",
    "\n",
    "# file_path = Path(rec_dir, 'test_subjcode_lst_z.csv')\n",
    "# SubjCodes = list(np.loadtxt(file_path, delimiter=',', dtype=\"int\"))\n",
    "\n",
    "print(\"Klasifikuojamų įrašų sąrašas:\", SubjCodes)\n",
    "print(f\"Sąrašas nuskaitytas iš: {file_path}\")\n",
    "\n",
    "# Suformuojamas aplankas rezultatams\n",
    "\n",
    "head_tail = os.path.split(file_path)\n",
    "filename, file_extension = os.path.splitext(head_tail[1])\n",
    "path_for_results = Path(db_path, 'rezultatai' + '_' + filename)\n",
    "print(f\"\\nAplankas rezultatams: {path_for_results}\")\n",
    "create_dir(path_for_results)\n",
    "\n",
    "# Kas kiek išvedamas apdorotų sekų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Klasių simbolinių vardų sąrašas ir klasių skaičius\n",
    "class_names = list(selected_beats.keys()) \n",
    "n_classes = len(selected_beats)\n",
    "# print(class_names)\n",
    "\n",
    "                    # SUFORMUOJAME all_beats_attr\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "annot_grouping = {'N':'N', 'S':'S', 'V':'V', 'U':'U'}\n",
    "\n",
    "# Sukūriame masyvą sekų atributų sąrašui\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "all_beats_attr = pd.DataFrame({'userNr': pd.Series(dtype='int'),\n",
    "                   'recordingNr': pd.Series(dtype='int'),\n",
    "                   'sample': pd.Series(dtype='int'),\n",
    "                   'symbol': pd.Series(dtype='str'),\n",
    "                   'label': pd.Series(dtype='int'),\n",
    "                   'pred_label': pd.Series(dtype='int')\n",
    "                   })\n",
    "\n",
    "df_rec_results = pd.DataFrame({'userNr':pd.Series(dtype='int'),\n",
    "    'recordingNr':pd.Series(dtype='int'), 'file_name':pd.Series(dtype='str'),  \n",
    "    'userId':pd.Series(dtype='str'), 'recId':pd.Series(dtype='int'), 'signal_length':pd.Series(dtype='int'),\n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'), 'U':pd.Series(dtype='int'),\n",
    "    'Nml':pd.Series(dtype='int'), 'Sml':pd.Series(dtype='int'), 'Vml':pd.Series(dtype='int'), 'Uml':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "     'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# CIKLAS PER PACIENTŲ ĮRAŠUS\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for SubjCode in SubjCodes:\n",
    "    # print(\"\\nECG įrašas:\", SubjCode)\n",
    "    \n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    atr_sample, atr_symbol = read_rec_attrib(rec_dir, SubjCode)\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "\n",
    "    # Nuskaitome EKG įrašą (npy formatu)\n",
    "    sign_raw = read_rec(rec_dir, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "    \n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    userId, recId, file_name = get_recId(rec_dir, userNr, recNr)\n",
    "    print(f\"\\nSubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2} userId: {userId} recId: {recId} signal_length: {signal_length}\")\n",
    "\n",
    "    # Filtruojame signalą\n",
    "    # signal = signal_filter(signal=sign_raw, sampling_rate=200, lowcut=0.2, method=\"butterworth\", order=5)\n",
    "\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "\n",
    "    label_sums, total = get_label_sums(test_labels, all_beats)  \n",
    "    # print(\"test_labels: \", list(all_beats.keys()), label_sums, \"Total:\", total)\n",
    "\n",
    "    # Surandame ML anotacijų skaitmenines reikšmes pred_labels\n",
    "    # Jei atr_symbol atranda anotaciją 'U', tai jos neklasifikuoja,\n",
    "    # bet iš karto patalpina '3' į pred_labels atitinkamą vietą.\n",
    "    # Į pred_labels taip pat įrašomas '3' pirmam ir paskutiniam pūpsniui,\n",
    "    # o taip pat pakliuvusiems į ommited sritį, \n",
    "    pred_labels = predict_cnn_fda_vu_v1_micro(signal, atr_sample, atr_symbol, model_dir)\n",
    "    \n",
    "    # pred_labels turi būti tokio pat ilgio, kaip ir test_labels\n",
    "    if (len(test_labels) != len(pred_labels)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_labels ir pred_labels ilgiai\")     \n",
    "\n",
    "    label_sums_ml, total = get_label_sums(pred_labels, all_beats)  \n",
    "    # print(\"pred_labels: \", list(all_beats.keys()), label_sums_ml, \"Total:\", total)\n",
    "\n",
    "    # Surandame vietas su ekstrasistolemis ir išvedame jų sąrašą vizualiniam įvertinimui. \n",
    "    classification=[]\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "        if ((pred_labels[i] != 0) or test_labels[i] != 0):\n",
    "            classification.append({'sample':i_sample, 'annot':test_labels[i], 'pred':pred_labels[i]})\n",
    "\n",
    "    # Vietų sąrašas išvedamas\n",
    "    # Dirbant su daug įrašų sąrašo išvedimą reikia užblokuoti !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    classification = []  # uzblokuota\n",
    "    if (classification):\n",
    "        print('\\nVietos su ekstrasistolėmis test_y arba pred_y:')\n",
    "        for row in classification:\n",
    "            print(f\"sample: {row['sample']:>7}   annot_label: {row['annot']:>2}   pred_label: {row['pred']:>2}\")  \n",
    "\n",
    "    # Ciklas per visas paciento įrašo anotacijas (simbolius) ir jų vietas (i_sample)\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "\n",
    "        # Formuojame pūpsnio atributus\n",
    "        beats_attr = {'userNr':int(userNr), 'recordingNr':int(recNr), 'sample':int(i_sample), \n",
    "                        'symbol':str(atr_symbol[i]), 'label':test_labels[i], 'pred_label':pred_labels[i]}\n",
    "\n",
    "        # Kaupiame su concat\n",
    "        df_new_row = pd.DataFrame([beats_attr])\n",
    "        all_beats_attr = pd.concat([all_beats_attr, df_new_row])\n",
    "\n",
    " # Suformuojame klasių numerių masyvus confusion matricai skaičiuoti, surandama confusion matrica\n",
    "\n",
    "    # pred_labels turi būti tokio pat ilgio, kaip ir test_labels\n",
    "    if (len(test_labels) != len(pred_labels)):\n",
    "        raise Exception(f\"Klaida! SubjCode: {SubjCode}. Nesutampa test_labels ir pred_labels ilgiai\")     \n",
    "\n",
    "    test_labels_mod, pred_labels_mod = get_rid_off_class_3(test_labels, pred_labels)\n",
    "    label_sums_ml_3, total = get_label_sums(pred_labels_mod, all_beats)  \n",
    "    # print(\"pred_labels_3: \", list(all_beats.keys()), label_sums_ml_3, \"Total:\", total)\n",
    "\n",
    "    confusion = confusion_matrix(test_labels_mod, pred_labels_mod)\n",
    "    # print()\n",
    "    # print(confusion)\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(test_labels_mod, pred_labels_mod, labels=[0, 1, 2], zero_division=0)\n",
    "\n",
    "    str1 =f\"N:{int(label_sums[0]):>5} S:{(int(label_sums[1])):3} V:{int(label_sums[2]):3} U:{int(label_sums[3]):3}\" \n",
    "    str2 =f\"  Nml:{int(label_sums_ml[0]):>5} Sml:{(int(label_sums_ml[1])):3} Vml:{int(label_sums_ml[2]):3} Uml:{int(label_sums_ml[3]):3}\" \n",
    "    str3 = f\"  Nprec:{prec[0]:>5.2f} Nrec:{rec[0]:5.2f} Nfsc:{fsc[0]:5.2f}\"\n",
    "    str4 = f\"  Sprec:{prec[1]:>5.2f} Srec:{rec[1]:5.2f} Sfsc:{fsc[1]:5.2f}\"\n",
    "    str5 = f\"  Vprec:{prec[2]:>5.2f} Vrec:{rec[2]:5.2f} Vfsc:{fsc[2]:5.2f}\"\n",
    "    # print()\n",
    "    print(str1+str2+str3+str4+str5)\n",
    "\n",
    "    dict_rec_results = {'userNr':int(userNr), 'recordingNr':recNr, 'file_name':file_name,\n",
    "    'userId': userId, 'recId': recId, 'signal_length': signal_length,\n",
    "    'N':label_sums[0], 'S':label_sums[1], 'V':label_sums[2], 'U':label_sums[3],\n",
    "    'Nml':label_sums_ml[0], 'Sml':label_sums_ml[1], 'Vml':label_sums_ml[2], 'Uml':label_sums_ml[3],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2] \n",
    "    }\n",
    "    rows_list.append(dict_rec_results)\n",
    "\n",
    "df_rec_results =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Ciklo per pacientų įrašus pabaiga\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\n')\n",
    "runtime(end_time-start_time)\n",
    "\n",
    "filepath = Path(path_for_results, 'klasifikacijos_rezultatai_irasams.csv') \n",
    "df_rec_results.to_csv(filepath)    \n",
    "print(f'\\nRezultatai įrašyti į:  {filepath}')\n",
    "\n",
    "# Pernumeruojame indeksus, kad būtų nuo 0 iš eilės\n",
    "all_beats_attr.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Įrašome sekos atributų masyvą į rec_dir aplanką\n",
    "file_path = Path(path_for_results, 'all_beats_attr.csv')\n",
    "all_beats_attr.to_csv(file_path)\n",
    "print(\"\\nAtributų freimas įrašytas: į \", file_path, \"\\n\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MODELIO TIKSLUMO VERTINIMO REZULTATAI\n",
      "Modelis iš aplanko:  model_cnn_fda_vu_v1\n",
      "Pūpsnių klasės:  ['N', 'S', 'V', 'U'] [3401    0  164  124] Total: 3689\n",
      "Klasifikuojamos klasės: ['N', 'S', 'V']\n",
      "\n",
      "APIBENDRINTI REZULTATAI\n",
      "\n",
      "Confusion Matrix\n",
      "      N  S    V\n",
      "N  3348  6   10\n",
      "S     0  0    0\n",
      "V     4  1  158\n",
      "\n",
      "\n",
      "Zero values! Cannot calculate Normalized Confusion Matrix\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N      0.999     0.995     0.997      3364\n",
      "           S      0.000     0.000     0.000         0\n",
      "           V      0.940     0.969     0.955       163\n",
      "\n",
      "    accuracy                          0.994      3527\n",
      "   macro avg      0.646     0.655     0.651      3527\n",
      "weighted avg      0.996     0.994     0.995      3527\n",
      "\n",
      "\n",
      "Apibendrinti_rezultatai įrašyti į:  D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z\\apibendrinti_rezultatai.csv\n"
     ]
    }
   ],
   "source": [
    "# MODELIO TIKSLUMO VERTINIMO IŠ VERTINIMO IMTIES REZULTATAI\n",
    "\n",
    "# Nuskaitome pūpsnių atributų masyvą\n",
    "file_path = Path(path_for_results, 'all_beats_attr.csv')\n",
    "all_beats_attr = pd.read_csv(file_path, index_col=0, dtype = {'userNr': int, 'recordingNr': int,\n",
    "                                            'sample': int, 'symbol': str, 'label': int, 'pred_label':int })\n",
    "\n",
    "# Sukūriame anotuotų ir automatiškai priskirtų klasių visų įrašų pūpsniams sąrašus \n",
    "validate_ind_lst = all_beats_attr.index\n",
    "y_validate = np.array(all_beats_attr['label']).astype('int')\n",
    "y_predicted = np.array(all_beats_attr['pred_label']).astype('int')\n",
    "\n",
    "print(\"\\nMODELIO TIKSLUMO VERTINIMO REZULTATAI\")\n",
    "print(\"Modelis iš aplanko: \", model_dir)\n",
    "\n",
    "label_sums, total = get_label_sums(y_validate, all_beats)  \n",
    "print(\"Pūpsnių klasės: \", list(all_beats.keys()), label_sums, \"Total:\", total)\n",
    "print(\"Klasifikuojamos klasės: ['N', 'S', 'V']\")\n",
    "y_validate_mod, y_predicted_mod = get_rid_off_class_3(y_validate, y_predicted)\n",
    "\n",
    "# APIBENDRINTI REZULTATAI\n",
    "\n",
    "print('\\nAPIBENDRINTI REZULTATAI\\n')\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++  čia reiktų įdėtiy_validate, y_predicted valymą nuo 3 klasės\n",
    "\n",
    "# Skaičiuojame ir išvedame klasifikavimo lentelę\n",
    "confusion = confusion_matrix(y_validate_mod, y_predicted_mod)\n",
    "pd.set_option('display.precision',3)\n",
    "show_confusion_matrix(confusion, class_names)\n",
    "# print('\\n')\n",
    "\n",
    "print(\"\\nClassification Report\\n\")\n",
    "# target_names = [key for (key, value) in selected_beats.items()]\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000)\n",
    "pd.set_option(\"display.max_columns\",200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(classification_report(y_validate_mod, y_predicted_mod, target_names=class_names, digits=3))\n",
    "report = classification_report(y_validate_mod, y_predicted_mod, target_names=class_names, output_dict=True)\n",
    "# output_dictbool, default=False, If True, return output as dict.\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "# https://medium.com/@asmaiya/you-can-something-like-this-84d28e0fd31f\n",
    "\n",
    "# Įrašome į diską\n",
    "filepath = Path(path_for_results, 'apibendrinti_rezultatai.csv') \n",
    "df_report.to_csv(filepath)    \n",
    "print(f'\\nApibendrinti_rezultatai įrašyti į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rezultatų pasiskirstymas per pacientus\n",
      "Pacientų: 2\n",
      "\n",
      "\n",
      "\n",
      "userNr                   userId       N    S    V    U     Nml  Sml  Vml  Uml   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "   105                      105    2526    0   41  124    2476    6   49  160    1.00  0.99  1.00    0.00  0.00  0.00    0.82  1.00  0.90      1.9      0.0\n",
      "  1019 6144c682bd0cc5acb7275368     875    0  123    0     876    1  119    2    1.00  1.00  1.00    0.00  0.00  0.00    0.99  0.96  0.98      0.8    100.0\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientus įrašytas į:  D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z\\rezultatu_pasiskirstymas_per_pacientus.csv\n"
     ]
    }
   ],
   "source": [
    "# KLAIDŲ PASISKIRSTYMAS PER PACIENTUS IR JŲ ĮRAŠUS\n",
    "\n",
    "\n",
    "def collect_noise_locs(rec_dir, all_beats_attr):\n",
    "\n",
    "    all_beat_indices = all_beats_attr.index\n",
    "\n",
    "    grouped = all_beats_attr.groupby(['userNr', 'recordingNr'])\n",
    "    for key, group in grouped:\n",
    "        # print('\\n',key)\n",
    "        userNr = key[0]\n",
    "        recordingNr = key[1]\n",
    "\n",
    "        # SubjCode naudojamas atveju, kai duomenis yra formoje SubjCode.npy, SubjCode.json\n",
    "        SubjCode = create_SubjCode(userNr, recordingNr)\n",
    "        filepath = Path(rec_dir, str(SubjCode) + '.json') \n",
    "        df_noises = zive_read_df_data(filepath, 'noises')\n",
    "        noise_arr = np.full(shape=len(group), fill_value=0,  dtype=int)\n",
    "        \n",
    "        if (not df_noises.empty):\n",
    "            idx_noise = 0\n",
    "            for i, row_i in group.iterrows():\n",
    "                sample = row_i['sample']\n",
    "                for j, row_j in df_noises.iterrows():\n",
    "                    if ((sample > row_j['startIndex']) & (sample < row_j['endIndex'])):\n",
    "                        noise_arr[idx_noise] = 1\n",
    "                    else:\n",
    "                        noise_arr[idx_noise] = 0\n",
    "                # print(idx_noise, noise_arr[idx_noise], sample)        \n",
    "                idx_noise += 1\n",
    "        \n",
    "        # Surandame pradinį SubjCode įrašo indeksą faile all_beats_attr\n",
    "        selected_ind = all_beat_indices[(all_beats_attr['userNr']==userNr) & (all_beats_attr['recordingNr']==recordingNr)]\n",
    "        index_start = selected_ind[0]\n",
    "        # print(f\"SubjCode: {SubjCode}  first elem: {selected_ind[0]} last elem: {selected_ind[-1]}  tot: {len(selected_ind)}\")\n",
    "        \n",
    "        for i in range(len(noise_arr)):\n",
    "            all_beats_attr.loc[all_beats_attr.index[index_start+i], 'noise'] = noise_arr[i]\n",
    "\n",
    "    # print(\"\\nall_beats_attr.groupby(['userNr', 'recordingNr'])['noise'].sum():\")\n",
    "    # print(all_beats_attr.groupby(['userNr', 'recordingNr'])[\"noise\"].sum())\n",
    "\n",
    "    # print(\"2: all_beats_attr.head():\")\n",
    "    # print(all_beats_attr.head())\n",
    "    # print(all_beats_attr.tail())\n",
    "\n",
    "    # return all_beats_attr\n",
    "    return all_beats_attr['noise'].values\n",
    "\n",
    "\n",
    "def get_error(y_test, y_pred):\n",
    "    # Error in %\n",
    "    n_errors = 0\n",
    "    for idx in range(len(y_pred)):\n",
    "        if (y_test[idx] != y_pred[idx]):\n",
    "            n_errors += 1\n",
    "    Err = float(n_errors)/len(y_pred)*100.\n",
    "    Err = round(Err, 1)\n",
    "    return Err\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 6000)\n",
    "pd.set_option(\"display.max_columns\", 15)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "\n",
    "\n",
    "             # Pagrindinis masyvas: df_seq_errors\n",
    "\n",
    "# Sukuriame ir užpildome dataframe su sekų parametrais\n",
    "df_seq_errors = pd.DataFrame(columns= ['idx', 'userNr', 'file_name'])\n",
    "# df_seq_errors = pd.DataFrame()\n",
    "\n",
    "rows_list = []\n",
    "for idx in validate_ind_lst:\n",
    "# Surandame  userNr, recordingNr, symbol\n",
    "    userNr, recNr, label, symbol = get_beat_attributes(idx, all_beats_attr)\n",
    "    seq_attr = {'idx': idx, 'userNr':userNr, 'recordingNr':recNr}\n",
    "    rows_list.append(seq_attr)\n",
    "\n",
    "# print(\"rows_list:\")\n",
    "# print(rows_list[:10])\n",
    "\n",
    "# Čia įdedame informaciją, ar pūpsnys patenka į triukšmų zoną\n",
    "noise_arr = collect_noise_locs(rec_dir, all_beats_attr)\n",
    "# noise_arr = noise_arr[validate_ind_lst]\n",
    "\n",
    "# print(\"3: all_beats_attr.head():\")\n",
    "# print(all_beats_attr.head())\n",
    "# print(all_beats_attr.tail())\n",
    "\n",
    "# print(type(noise_arr) , noise_arr)\n",
    "\n",
    "df_seq_errors = pd.DataFrame(rows_list)\n",
    "\n",
    "df_seq_errors['labels'] = pd.Series(y_validate)\n",
    "df_seq_errors['preds'] = pd.Series(y_predicted)\n",
    "zeros_arr = np.zeros( y_predicted.shape[0], dtype=int)\n",
    "df_seq_errors['errors'] = pd.Series(zeros_arr)  \n",
    "df_seq_errors.loc[df_seq_errors['labels'] != df_seq_errors['preds'], 'errors'] = 1 \n",
    "df_seq_errors['noises'] = pd.Series(noise_arr)  \n",
    "\n",
    "# print(\"\\ndf_seq_errors:\")\n",
    "# print(df_seq_errors.head())\n",
    "# print(df_seq_errors.tail())\n",
    "# # print(df_seq_errors.info())\n",
    "\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientus\n",
    "                # Pagrindinis masyvas : df_user_errors\n",
    "\n",
    "print(\"\\nRezultatų pasiskirstymas per pacientus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_user_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'), 'userId':pd.Series(dtype='str'),  # ??????????????????????\n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "    'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Išgauname pacientų vidinę (eilės nr) numeraciją\n",
    "grouped  = df_seq_errors.groupby(['userNr'])\n",
    "userNrs = list(grouped.groups.keys())\n",
    "print(f'Pacientų: {len(userNrs)}\\n')\n",
    "\n",
    "for userNr in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[userNr]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[userNr]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "    \n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[userNr]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "    # Testavimui\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++  čia reiktų įdėti y_test, y_pred valymą nuo 3 klasės\n",
    "    label_sums, total = get_label_sums(y_test, all_beats)  \n",
    "    # print(\"test_labels: \", list(all_beats.keys()), label_sums, \"Total:\", total)\n",
    "\n",
    "    y_test_mod, y_pred_mod = get_rid_off_class_3(y_test, y_pred)\n",
    "    \n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test_mod, y_pred_mod, labels=[0, 1, 2], zero_division=0)\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    label_sums_ml, total = get_label_sums(y_pred, all_beats)  \n",
    "    # print(\"pred_labels: \", list(all_beats.keys()), label_sums_ml, \"Total:\", total)\n",
    "\n",
    "    dict_user_errors = {'userNr':userNr,'userId':str(userId),\n",
    "    'N':label_sums[0], 'S':label_sums[1], 'V':label_sums[2], 'U':label_sums[3],\n",
    "    'Nml':label_sums_ml[0], 'Sml':label_sums_ml[1], 'Vml':label_sums_ml[2], 'Uml':label_sums_ml[3],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err,  'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_user_errors)\n",
    "\n",
    "df_user_errors =  pd.DataFrame(rows_list)\n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "tit1 = f\"{'userNr':>6} {'userId':>24}\"\n",
    "tit2 = f\"{'N':>8} {'S':>4} {'V':>4} {'U':>4}\"\n",
    "tit3 = f\"{'Nml':>8} {'Sml':>4} {'Vml':>4} {'Uml':>4}\"\n",
    "tit4 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit5 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit6 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print('\\n')\n",
    "print(tit1+tit2+tit3+tit4+tit5+tit6)\n",
    "\n",
    "for idx, row in  df_user_errors.iterrows():\n",
    "    userId = row['userId']\n",
    "    if (len(userId)) < 4:\n",
    "        str1 = f\"{int(row['userNr']):>6} {userId:>24}\"\n",
    "    else:\n",
    "        str1 = f\"{int(row['userNr']):>6} {userId:>6}\"\n",
    "    # print(userId, type(userId))\n",
    "    # mark2\n",
    "    str2 = f\"{int(row['N']):>8} {int(row['S']):4} {int(row['V']):4} {int(row['U']):4}\" \n",
    "    str3 = f\"{int(row['Nml']):>8} {int(row['Sml']):4} {int(row['Vml']):4} {int(row['Uml']):4}\" \n",
    "    str4 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "    str5 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "    str6 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "    print(str1+str2+str3+str4+str5+str6)\n",
    "\n",
    "filepath = Path(path_for_results, 'rezultatu_pasiskirstymas_per_pacientus.csv') \n",
    "df_user_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per pacientus įrašytas į:  {filepath}')\n",
    "\n",
    "# print(df_user_errors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Rezultatų pasiskirstymas per pacientų įrašus\n",
      "Pacientų įrašų: 2\n",
      "\n",
      "\n",
      "userNr: 105 userId: 105\n",
      "recordingNr                    recId       file_name       N    S    V    U     Nml  Sml  Vml  Uml   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     0                             0         105.npy    2526    0   41  124    2476    6   49  160    1.00  0.99  1.00    0.00  0.00  0.00    0.82  1.00  0.90      1.9      0.0\n",
      "\n",
      "\n",
      "userNr: 1019 userId: 6144c682bd0cc5acb7275368\n",
      "recordingNr                    recId       file_name       N    S    V    U     Nml  Sml  Vml  Uml   Nprec  Nrec  Nfsc   Sprec  Srec  Sfsc   Vprec  Vrec  Vfsc     Err%   Noise%\n",
      "     0      6145fd97bd0cc5ed18275bc8     1631036.053     875    0  123    0     876    1  119    2    1.00  1.00  1.00    0.00  0.00  0.00    0.99  0.96  0.98      0.8    100.0\n",
      "\n",
      "Rezultatų pasiskirstymas per įrašus įrašytas į:  D:\\DI\\DUOM_2022_RUDUO\\rezultatai_info_create_z\\rezultatu_pasiskirstymas_per_irasus.csv\n"
     ]
    }
   ],
   "source": [
    "# Rezultatų pasiskirstymas per pacientus ir jų įrašus\n",
    "# Skaičiuojama visoms 3 klasėms Precision(tikslumas), Recall (atgaminimas), Fscore (F rodiklis)\n",
    "# \n",
    "# https://towardsdatascience.com/you-dont-always-have-to-loop-through-rows-in-pandas-22a970b347ac\n",
    "# \n",
    "pd.set_option(\"display.max_rows\", 6000)\n",
    "pd.set_option(\"display.max_columns\", 18)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Rezultatų pasiskirstymas per pacientų įrašus\n",
    "print(\"\\n\\nRezultatų pasiskirstymas per pacientų įrašus\")\n",
    "\n",
    "# Pasiruošimas\n",
    "class_names = ['N', 'S', 'V']\n",
    "n_classes = len(class_names)\n",
    "df_rec_errors = pd.DataFrame({'userNr':pd.Series(dtype='int'),\n",
    "    'recordingNr':pd.Series(dtype='int'), 'recId':pd.Series(dtype='str'), 'file_name':pd.Series(dtype='str'),  \n",
    "    'N':pd.Series(dtype='int'), 'S':pd.Series(dtype='int'), 'V':pd.Series(dtype='int'), 'U':pd.Series(dtype='int'),\n",
    "    'Nml':pd.Series(dtype='int'), 'Sml':pd.Series(dtype='int'), 'Vml':pd.Series(dtype='int'), 'Uml':pd.Series(dtype='int'),\n",
    "    'Nprec':pd.Series(dtype='float') , 'Nrec':pd.Series(dtype='float'), 'Nfsc':pd.Series(dtype='float'),\n",
    "    'Sprec':pd.Series(dtype='float') , 'Srec':pd.Series(dtype='float'), 'Sfsc':pd.Series(dtype='float'),\n",
    "    'Vprec':pd.Series(dtype='float') , 'Vrec':pd.Series(dtype='float'), 'Vfsc':pd.Series(dtype='float'), \n",
    "     'Err%':pd.Series(dtype='float'), 'Noise%':pd.Series(dtype='float')})\n",
    "# Pandas Empty DataFrame with Specific Column Types\n",
    "# https://sparkbyexamples.com/pandas/pandas-empty-dataframe-with-specific-column-types/\n",
    "\n",
    "rows_list = []\n",
    "\n",
    "# Sugrupuojame eilutes pagal 'userId','recordingId', suskaičiuojame, kiek kiekviename įraše\n",
    "# iš viso yra klasifikuojamų sekų (labels) ir kiek padaryta klaidų (errors).\n",
    "# Grupavimo objektą paverčiame į normalų dataframe objektą\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','recordingNr'])\n",
    "print(f'Pacientų įrašų: {grouped.ngroups}')\n",
    "# print(f'{grouped.size()=}')\n",
    "\n",
    "for key in grouped.groups:\n",
    "# https://stackoverflow.com/questions/62041850/looping-over-pandas-groupby-output-when-grouping-by-multiple-columns-and-missin\n",
    "\n",
    "    # print(f'\\nGroup: {key}\\n{df_seq_errors.loc[grouped.groups[key]]}')\n",
    "    userNr = key[0]\n",
    "    recordingNr = key[1]\n",
    "    userId, recId, file_name = get_recId(rec_dir, userNr, recordingNr)\n",
    "    # file_name = get_filename(rec_dir, create_SubjCode(userNr, recordingNr))\n",
    "    # userId = get_userId(rec_dir, userNr)\n",
    "\n",
    "    y_test = df_seq_errors.loc[grouped.groups[key]]['labels'].to_numpy(dtype=int)\n",
    "    y_pred = df_seq_errors.loc[grouped.groups[key]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "    Err = get_error(y_test, y_pred)\n",
    "\n",
    "    noise_arr = df_seq_errors.loc[grouped.groups[key]]['noises'].to_numpy(dtype=int)\n",
    "    Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "\n",
    "    # *********************** Testavimui *******************************************************************\n",
    "    # acc = accuracy_score(y_test, y_pred)\n",
    "    # print(f\"userNr: {userNr} recordingNr:  {recordingNr} Accuracy: {acc:.2f}\")\n",
    "    # cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "    # show_confusion_matrix(cnf_matrix, class_names)\n",
    "    # https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "    # *********************************************************************************************************\n",
    "\n",
    "# +++++++++++++++++++++++++++++++++++  čia reiktų įdėti y_test, y_pred valymą nuo 3 klasės\n",
    "    label_sums, total = get_label_sums(y_test, all_beats)  \n",
    "    # print(\"test_labels: \", list(all_beats.keys()), label_sums, \"Total:\", total)\n",
    "\n",
    "    y_test_mod, y_pred_mod = get_rid_off_class_3(y_test, y_pred)\n",
    "\n",
    "    prec,rec,fsc,sup = precision_recall_fscore_support(y_test_mod, y_pred_mod, labels=[0, 1, 2], zero_division=0)\n",
    "    \n",
    "    label_sums_ml, total = get_label_sums(y_pred, all_beats)  \n",
    "    # print(\"pred_labels: \", list(all_beats.keys()), label_sums_ml, \"Total:\", total)\n",
    "\n",
    "    dict_rec_errors = {'userNr':int(userNr), 'recordingNr':recordingNr, 'recId':recId, 'file_name':file_name,\n",
    "    'N':label_sums[0], 'S':label_sums[1], 'V':label_sums[2], 'U':label_sums[3],\n",
    "    'Nml':label_sums_ml[0], 'Sml':label_sums_ml[1], 'Vml':label_sums_ml[2], 'Uml':label_sums_ml[3],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "    rows_list.append(dict_rec_errors)\n",
    "\n",
    "df_rec_errors =  pd.DataFrame(rows_list) \n",
    "\n",
    "# Išvedame suformuotą masyvą\n",
    "grouped  = df_rec_errors.groupby('userNr')\n",
    "for userNr, group in grouped:\n",
    "    # print(group.dtypes)\n",
    "    print(\"\\n\")\n",
    "    userId = get_userId(rec_dir, userNr)\n",
    "    print(f\"{'userNr:'} {userNr} {'userId:'} {userId}\" )\n",
    "    tit1 = f\"{'recordingNr':>6}  {'recId':>23} {'file_name':>15}\"\n",
    "    tit2 = f\"{'N':>8} {'S':>4} {'V':>4} {'U':>4}\"\n",
    "    tit3 = f\"{'Nml':>8} {'Sml':>4} {'Vml':>4} {'Uml':>4}\"\n",
    "    tit4 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "    tit5 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "    tit6 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "    print(tit1+tit2+tit3+tit4+tit5+tit6)\n",
    "\n",
    "    for idx, row in group.iterrows():\n",
    "        str1 =f\"{int(row['recordingNr']):>6} {str(row['recId']):>29} {str(row['file_name']):>15}\"\n",
    "        str2 = f\"{int(row['N']):>8} {int(row['S']):4} {int(row['V']):4} {int(row['U']):4}\" \n",
    "        str3 =f\"{int(row['Nml']):>8} {int(row['Sml']):4} {int(row['Vml']):4} {int(row['Uml']):4}\" \n",
    "        str4 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "        str5 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "        str6 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "        print(str1+str2+str3+str4+str5+str6)\n",
    "\n",
    "filepath = Path(path_for_results, 'rezultatu_pasiskirstymas_per_irasus.csv') \n",
    "df_rec_errors.to_csv(filepath)    \n",
    "print(f'\\nRezultatų pasiskirstymas per įrašus įrašytas į:  {filepath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bendras aplankas vaizdams: CNN_VU_vasara\n",
      "\n",
      "Max sekų vaizdų skaičius iš kiekvienos klasės: 10\n",
      "Directory 'D:\\DI\\DUOM_2022_RUDUO\\records_npy\\saved_images\\CNN_VU_vasara' already exists\n",
      "Directory 'D:\\DI\\DUOM_2022_RUDUO\\records_npy\\saved_images\\CNN_VU_vasara\\N' already exists\n",
      "Directory 'D:\\DI\\DUOM_2022_RUDUO\\records_npy\\saved_images\\CNN_VU_vasara\\S' already exists\n",
      "Directory 'D:\\DI\\DUOM_2022_RUDUO\\records_npy\\saved_images\\CNN_VU_vasara\\V' already exists\n",
      "klaida!\n",
      "Sekai 0 negali suformuoti išplėstinio vaizdo\n",
      "klaida!\n",
      "Sekai 1 negali suformuoti išplėstinio vaizdo\n",
      "klaida!\n",
      "Sekai 2 negali suformuoti išplėstinio vaizdo\n",
      "klaida!\n",
      "Sekai 3 negali suformuoti išplėstinio vaizdo\n",
      "klaida!\n",
      "Sekai 4 negali suformuoti išplėstinio vaizdo\n",
      "klaida!\n",
      "Sekai 5 negali suformuoti išplėstinio vaizdo\n",
      "\n",
      "\n",
      "    N  S   V\n",
      "N  10  6  10\n",
      "S   0  0   0\n",
      "V   4  1  10\n",
      "\n",
      "Pabaiga.........\n"
     ]
    }
   ],
   "source": [
    "# Sukuriami užduoto ilgio sekų vaizdai ir įrašomi į disko atitinkamus klasėms subfolderius\n",
    "\n",
    "# Reikalingi parametrai:\n",
    "# - all_beats_attr, suformuojamas automatiškai iš SubjCodes testinių kodų\n",
    "# - sets_path - aplankas, kuriame bus aplankas 'saved_images' su vaizdais. Šiuo atveju\n",
    "# sets_path = rec_dir   \n",
    "# wl_side, wr_side - analizei naudojami langai\n",
    "\n",
    "\n",
    "def read_show_seq_ext_zive_npy(rec_dir, all_beats_attr, idx, win_ls, win_rs, win_ls_ext, win_rs_ext):\n",
    "# Išpjauna užduoto ilgio seką iš mit2zive įrašo ir sukuria jos vaizdą su anotacijomis\n",
    "\n",
    "# rec_dir - paciento EKG įrašų aplankas\n",
    "# recordingId - paciento EKG įrašo Id - int\n",
    "# i_sample - R dantelio, kurio atžvilgiu formuojama seka, indeksas viso EKG įrašo reikšmių masyve - int\n",
    "# win_ls - klasifikuojamo EKG segmento plotis iki R pūpsnio (iš kairės) \n",
    "# win_rs - klasifikuojamo EKG segmento plotis nuo R pūpsnio (iš dešinės)\n",
    "# win_ls_ext - vaizduojamo EKG segmento plotis iki R pūpsnio (iš kairės) \n",
    "# win_rs_ext - vaizduojamo EKG segmento plotis už R pūpsnio (iš dešinės) \n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    sequence, sample, label = read_seq(rec_dir, all_beats_attr, idx, win_ls_ext, win_rs_ext)\n",
    "    if (sample == None):\n",
    "        print(\"klaida!\")\n",
    "        return None\n",
    "    \n",
    "    seq_start = sample - win_ls_ext\n",
    "    seq_end = sample + win_ls_ext\n",
    "\n",
    "    SubjCode = get_SubjCode(idx, all_beats_attr)\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    atr_sample, atr_symbol = read_rec_attrib(rec_dir, SubjCode)\n",
    "\n",
    "    # # suformuojame anotacijų žymes\n",
    "    beat_symbols, beat_locs = get_symbol_list(atr_symbol,atr_sample, seq_start, seq_end)\n",
    "\n",
    "    # deltax ir deltay simbolių pozicijų koregavimui\n",
    "    min = np.amin(sequence)\n",
    "    max = np.amax(sequence)\n",
    "    deltay = (max - min)/20\n",
    "    deltax = len(sequence)/100\n",
    "\n",
    "    # suformuojame vaizdą\n",
    "    x = np.arange(0, len(sequence), 1)\n",
    "    ax.plot(x, sequence, color=\"#6c3376\", linewidth=2)\n",
    "    left_mark = win_ls_ext - win_ls\n",
    "    right_mark = win_ls_ext + win_rs\n",
    "    ax.axvline(x = left_mark, color = 'b', linestyle = 'dotted')\n",
    "    ax.axvline(x = right_mark, color = 'b', linestyle = 'dotted')\n",
    "    for i in range(len(beat_locs)):\n",
    "        ax.annotate(beat_symbols[i],(beat_locs[i]-deltax,sequence[beat_locs[i]]+deltay))\n",
    "    ax.set_ylim([min, max+2*deltay])\n",
    "    \n",
    "    return(ax)\n",
    "\n",
    "def get_seq_attributes(all_beats_attr, idx):\n",
    "    # 'userNr', 'recordingNr', 'sample', 'symbol', 'label'\n",
    "    row = all_beats_attr.loc[idx]\n",
    "    return list(row) \n",
    "\n",
    "# ////////////////////// Užduodami parametrai /////////////////////\n",
    "\n",
    "# Bendras aplankas vaizdams\n",
    "images_folder = 'CNN_VU_vasara'\n",
    "sets_path = rec_dir\n",
    "\n",
    "# nurodome, kiek reikšmių naudojama prieš R dantelį ir po\n",
    "wl_side, wr_side = 100, 100\n",
    "\n",
    "# užduodame, kiek reikšmių vaizduosime prieš R dantelį ir po\n",
    "wl_side_ext = 720\n",
    "wr_side_ext = 720\n",
    "\n",
    "# Užduodame, kiek sekų vaizdų iš kiekvienos klasės įrašysime į diską\n",
    "img_max = 10\n",
    "\n",
    "# /////////////////////////////////////////////////////////////////\n",
    "\n",
    "print(\"\\nBendras aplankas vaizdams:\", images_folder)\n",
    "print(\"\\nMax sekų vaizdų skaičius iš kiekvienos klasės:\", img_max)\n",
    "\n",
    "if (img_max == 0):\n",
    "    sys.exit()\n",
    "\n",
    "# sukuriame bendrą aplanką vaizdams\n",
    "images_dir = Path(sets_path, 'saved_images', images_folder)\n",
    "create_dir(images_dir)\n",
    "\n",
    "# klasių simbolinių vardų sąrašas\n",
    "class_names = selected_beats.keys()\n",
    "\n",
    "# sukuriame aplankus sekų vaizdams klasėse\n",
    "create_subdir(images_dir, class_names)\n",
    "\n",
    "# Sukuriame vaizdų klasėse skaitiklį\n",
    "n_classes = len(class_names)\n",
    "skait = np.zeros((n_classes, n_classes), dtype=int)\n",
    "# print(skait)\n",
    "\n",
    "# sukuriame selected_beats reversiją\n",
    "rev_dict = get_rev_dictionary(selected_beats)\n",
    "\n",
    "y_test = df_seq_errors['labels'].to_numpy(dtype=int)\n",
    "y_pred = df_seq_errors['preds'].to_numpy(dtype=int)\n",
    "\n",
    "y_test_mod, y_pred_mod = get_rid_off_class_3(y_test, y_pred)\n",
    "\n",
    "# *********************** derinimui *******************************\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(f\"\\nAccuracy: {acc:.2f}\\n\")\n",
    "# cnf_matrix = confusion_matrix_modified(y_test, y_pred, n_classes)\n",
    "# show_confusion_matrix(cnf_matrix, class_names)\n",
    "# ******************************************************************\n",
    "\n",
    "# Ciklas per sekas\n",
    "\n",
    "icycle = 0 \n",
    "leng = len(y_pred_mod)\n",
    "\n",
    "for idx in validate_ind_lst:\n",
    "    if (icycle >= leng):\n",
    "        continue\n",
    "\n",
    "# *********************** derinimui ******************************   \n",
    "    # row = get_seq_attributes(all_beats_attr, idx)\n",
    "    # print(idx, row)\n",
    "# ****************************************************************\n",
    "\n",
    "    # anotuotos klasės (klasių nr ir simboliniai pažymėjimai)\n",
    "    label = y_test_mod[icycle]\n",
    "    label_symb = rev_dict[label]\n",
    "\n",
    "    # print(f\" {idx} label {label} symb {label_symb}\")\n",
    "\n",
    "    # klasifikatoriaus priskirtos klasės (klasių nr ir simboliniai pažymėjimai)\n",
    "    pred = y_pred_mod[icycle]\n",
    "    pred_symb = rev_dict[pred]\n",
    "    icycle +=1\n",
    "\n",
    "    # print(f\" {idx} pred {pred} symb {pred_symb}\")\n",
    "\n",
    "    # patikriname, ar neviršytas skaitiklis, jei viršytas, peršokame\n",
    "    if (skait[label,pred] >= img_max):\n",
    "        continue\n",
    "    else:\n",
    "        skait[label,pred] += 1\n",
    "\n",
    "    SubjCode = get_SubjCode(idx, all_beats_attr)\n",
    "    seq_name = str(SubjCode) + '_' + str(idx)\n",
    "    \n",
    "     # 'Išpjauname' užduoto ilgio sekas ir sukuriame jų vaizdus\n",
    "    fig = plt.figure(facecolor=(1, 1, 1), figsize=(18,3))\n",
    "    # print(\"rec_dir =\", rec_dir)\n",
    "    ax = read_show_seq_ext_zive_npy(rec_dir, all_beats_attr, idx, wl_side, wr_side, wl_side_ext, wr_side_ext) \n",
    "    if (ax == None):\n",
    "        print(f'Sekai {idx} negali suformuoti išplėstinio vaizdo')\n",
    "        plt.close()\n",
    "        continue\n",
    "\n",
    "    # suformuosime koreguotą failo vardą\n",
    "    file_name_orig = get_rec_file_name(rec_dir, SubjCode)\n",
    "               \n",
    "    # suformuosime koreguotą failo vardą\n",
    "    if (file_name_orig == None):\n",
    "        file_name = seq_name + '_' + 'pred_' + pred_symb + \".png\" \n",
    "    else:\n",
    "        file_name = str(file_name_orig) + '_' + seq_name + '_' + 'pred_' + pred_symb + \".png\" \n",
    "    \n",
    "    # suformuosime kelią į atitinkamą sub-aplanką\n",
    "    image_subdir = Path(images_dir, label_symb)\n",
    "    if (os.path.exists(image_subdir) == False):\n",
    "        print('Klaida! ', image_subdir,' neegzistuoja')\n",
    "    file_path = Path(image_subdir, file_name)\n",
    "    # print('file_name: ',file_name, 'file_path: ', file_path)   \n",
    "\n",
    "    # Įrašome į atitinkamą anotacijai sub-aplanką \n",
    "    ax.set_title(file_name)\n",
    "    plt.savefig(file_path, bbox_inches='tight', pad_inches = 0.2)\n",
    "    plt.close()\n",
    "\n",
    "    # if (icycle >= len(y_pred)):\n",
    "        # break\n",
    "\n",
    "# ciklo per validate_ind_lst pabaiga\n",
    "\n",
    "print(\"\\n\")\n",
    "df = cm2df(skait, class_names)\n",
    "print(df)\n",
    "\n",
    "print(\"\\nPabaiga.........\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zive įrašas:\n",
      "userNr: 300  recordingNr: 0\n",
      "\n",
      "userId: 300  file_name: 300.npy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(300, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22508\\3179270622.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mgrouped\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdf_seq_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userNr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'recordingNr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_seq_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserNr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecordingNr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_seq_errors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgrouped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserNr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecordingNr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (300, 0)"
     ]
    }
   ],
   "source": [
    "# Klasifikavimo rodiklių skaičiavimas vienam nurodytam įrašui - įrašas turi būti iš testinio sąrašo\n",
    "\n",
    "userNr, recordingNr = 300, 0\n",
    "\n",
    "userId, recId, file_name =  get_recId(rec_dir, userNr, recordingNr)\n",
    "print(\"\\nZive įrašas:\")\n",
    "print(f\"userNr: {userNr}  recordingNr: {recordingNr}\")\n",
    "print(f\"\\nuserId: {userId}  file_name: {file_name}\")\n",
    "\n",
    "grouped  = df_seq_errors.groupby(['userNr','recordingNr'])\n",
    "y_test = df_seq_errors.loc[grouped.groups[(userNr,recordingNr)]]['labels'].to_numpy(dtype=int)\n",
    "y_pred = df_seq_errors.loc[grouped.groups[(userNr,recordingNr)]]['preds'].to_numpy(dtype=int)\n",
    "\n",
    "print(\"\\nAnotacijų pasiskirstymas įraše:\")\n",
    "label_sums, total = get_label_sums(y_test, all_beats)  \n",
    "print(\"test_labels: \", list(all_beats.keys()), label_sums, \"Total:\", total)\n",
    "label_sums_ml, total = get_label_sums(y_pred, all_beats)  \n",
    "print(\"pred_labels: \", list(all_beats.keys()), label_sums_ml, \"Total:\", total)\n",
    "\n",
    "noise_arr = df_seq_errors.loc[grouped.groups[(userNr,recordingNr)]]['noises'].to_numpy(dtype=int)\n",
    "Noise = np.sum(noise_arr, axis=0)/noise_arr.shape[0]*100.\n",
    "\n",
    "Err = get_error(y_test, y_pred)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f} Error(%): {Err}\\n \")\n",
    "\n",
    "y_test_mod, y_pred_mod = get_rid_off_class_3(y_test, y_pred)\n",
    "\n",
    "cnf_matrix = confusion_matrix_modified(y_test_mod, y_pred_mod, n_classes)\n",
    "show_confusion_matrix(cnf_matrix, class_names)\n",
    "# https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "# *********************************************************************************************************\n",
    "\n",
    "prec,rec,fsc,sup = precision_recall_fscore_support(y_test_mod, y_pred_mod, labels=[0, 1, 2], zero_division=0)\n",
    "row = {\n",
    "    'N':label_sums[0], 'S':label_sums[1], 'V':label_sums[2], 'U':label_sums[3],\n",
    "    'Nml':label_sums_ml[0], 'Sml':label_sums_ml[1], 'Vml':label_sums_ml[2], 'Uml':label_sums_ml[3],\n",
    "    'Nprec':prec[0], 'Nrec':rec[0], 'Nfsc':fsc[0],\n",
    "    'Sprec':prec[1], 'Srec':rec[1], 'Sfsc':fsc[1],\n",
    "    'Vprec':prec[2], 'Vrec':rec[2], 'Vfsc':fsc[2], \n",
    "     'Err%':Err, 'Noise%':Noise\n",
    "    }\n",
    "\n",
    "print(\"\\n\")\n",
    "tit1 = f\"{'N':>8} {'S':>4} {'V':>4} {'U':>4}\"\n",
    "tit2 = f\"{'Nml':>8} {'Sml':>4} {'Vml':>4} {'Uml':>4}\"\n",
    "tit3 = f\"{'Nprec':>8} {'Nrec':>5} {'Nfsc':>5}\"\n",
    "tit4 = f\"{'Sprec':>8} {'Srec':>5} {'Sfsc':>5}\"\n",
    "tit5 = f\"{'Vprec':>8} {'Vrec':>5} {'Vfsc':>5} {'Err%':>8} {'Noise%':>8}\"\n",
    "print(tit1+tit2+tit3+tit4+tit5)\n",
    "\n",
    "str1 =f\"{int(row['N']):>8} {int(row['S']):4} {int(row['V']):4} {int(row['U']):4}\" \n",
    "str2 =f\"{int(row['Nml']):>8} {int(row['Sml']):4} {int(row['Vml']):4} {int(row['Uml']):4}\" \n",
    "str3 = f\"{row['Nprec']:>8.2f} {row['Nrec']:5.2f} {row['Nfsc']:5.2f}\"\n",
    "str4 = f\"{row['Sprec']:>8.2f} {row['Srec']:5.2f} {row['Sfsc']:5.2f}\"\n",
    "str5 = f\"{row['Vprec']:>8.2f} {row['Vrec']:5.2f} {row['Vfsc']:5.2f} {row['Err%']:8.1f} {row['Noise%']:8.1f}\"\n",
    "print(str1+str2+str3+str4+str5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f36dab35816871602f0a4fffa6415a4e758bca001397bb3d9f7e90aab6637a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
