{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testuojama, bandant suprasti, kaip kuriamos partijos (batches) su tf.data.dataset \n",
    "# ir kaip apskaičiuoti MEAN ir STD naudojant tf.data.Dataset\n",
    "# tf.data.Dataset objektas sukūriamas darbui su csv failu\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys, os, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Funkcija csv failo eilučių skaičiau suradimui\n",
    "def count_lines_enumrate(file_name):\n",
    "    fp = open(file_name,'r')\n",
    "    for line_count, line in enumerate(fp):\n",
    "        pass\n",
    "    return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testuojama, bandant suprasti, kaip kuriamos partijos (batches) su tf.data.dataset \n",
    "# Sukūriame tf.data.dataset klasės objektą duomenų skaitymui partijomis iš csv failo\n",
    "# Naudojamas train_features.csv, sukūrtas su 8_1_zive_create_feature_sets_cnn_vu_v1_micro.ipynb\n",
    "\n",
    "        # Susikuriame failą bandymams su nauju stulpeliu 'nr' \n",
    "\n",
    "filepath = \"train_features.csv\"\n",
    "print()\n",
    "print(f'\\nPožymių masyvas iš:  {filepath}')\n",
    "\n",
    "# Eilučių skaičius csv masyve\n",
    "number_of_rows = count_lines_enumrate(filepath)\n",
    "print(f'\\nEilučių skaičius csv masyve: {number_of_rows}')\n",
    "\n",
    "data_frame_tst = pd.read_csv(filepath)\n",
    "len_df = len(data_frame_tst) # number of rows\n",
    "#create NumPy array for column 'nr'\n",
    "blocks = np.array(range(len_df), int)\n",
    "\n",
    "#add 'nr' array as new column in DataFrame\n",
    "data_frame_tst['nr'] = blocks.tolist()\n",
    "\n",
    "filepath = \"train_features1.csv\"\n",
    "print(f'\\nPožymių masyvas iš:  {filepath}')\n",
    "# print()\n",
    "# print(data_frame.head(20))\n",
    "data_frame_tst.to_csv(filepath, index=False, header=True)\n",
    "\n",
    "# print()\n",
    "# data_frame.info()\n",
    "\n",
    "print()\n",
    "# data_frame_tst = data_frame_tst[selected_features]\n",
    "# print(data_frame_tst.head(20)) \n",
    "\n",
    "\n",
    "#        # Sukūriame tf.data.dataset klasės objektą duomenų skaitymui partijomis iš csv failo\n",
    "\n",
    "# filepath = Path(features_dir, feature_set_fname)\n",
    "filepath = \"train_features1.csv\"\n",
    "print()\n",
    "print(f'\\nPožymių masyvas iš:  {filepath}')\n",
    "batch_size = 5\n",
    "\n",
    "selected_columns=['nr','RRl/RRr', '0']\n",
    "\n",
    "dt_train_raw = tf.data.experimental.make_csv_dataset(\n",
    "    filepath,\n",
    "    batch_size=batch_size, # Artificially small to make examples easier to show.\n",
    "    select_columns=selected_columns,\n",
    "    label_name='nr',\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True,)\n",
    "\n",
    "# Kontrolinis skaitymas\n",
    "# for batch, label in dt_train_raw.take(2):\n",
    "#     # for key, value in batch.items():\n",
    "#     #   print(f\"{key:20s}: {value}\")\n",
    "#     print()\n",
    "#     print(f\"{'y':20s}: {label}\")  \n",
    "\n",
    "# Here's a simple function that will pack together all the columns:\n",
    "def pack(features, label):\n",
    "    return tf.stack(list(features.values()), axis=-1), label\n",
    "\n",
    "# Apply this to each element of the dataset:\n",
    "dt_train = dt_train_raw.map(pack)\n",
    "\n",
    "\n",
    "#         Kontrolinis skaitymas stebint 'nr' pasiskirstymą partijose\n",
    "\n",
    "for features, labels in dt_train.take(3):\n",
    "    print()\n",
    "    print(features.numpy())\n",
    "    print()\n",
    "    print(labels.numpy())\n",
    "\n",
    "\n",
    "# file = object.myfilePath\n",
    "# fileObject = csv.reader(file)\n",
    "# row_count = sum(1 for row in fileObject)  # fileObject is your csv.reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apskaičiuojame MEAN ir STD iš atsitiktinių partijų. Skaičiuojama \n",
    "# atsitiktiniu būdu generuojant partijas. Ar pereinama per visas\n",
    "# eilutes, priklauso nuo esančio take(N) skaičiaus\n",
    "\n",
    "print(f'\\nMEAN ir STD skaičiavimas naudojant tf.data.dataset')\n",
    "take_N = 12\n",
    "length = len(selected_columns)\n",
    "\n",
    "rate = 100*batch_size*take_N/number_of_rows\n",
    "print(f'\\nnumber_of_rows: {number_of_rows} take_N: {take_N} batch_size: {batch_size}  rate: {rate:.1f}%'  )\n",
    "\n",
    "np_features_set = np.empty((0,length-1),float)\n",
    "print(np_features_set, np_features_set.shape)\n",
    "\n",
    "for features, labels in dt_train.take(take_N):\n",
    "    block = features.numpy()\n",
    "    print()\n",
    "    print(block)\n",
    "    print()\n",
    "    print(labels.numpy())\n",
    "\n",
    "    np_features_set= np.append(np_features_set, block, axis=0)\n",
    "    print(np_features_set, np_features_set.shape)\n",
    "\n",
    "MEAN = np.mean(np_features_set, axis=0)\n",
    "STD = np.std(np_features_set, axis=0)\n",
    "print(f'\\nMEAN: {MEAN}')\n",
    "print(f'STD: {STD}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEAN ir STD iš numpy skaičiavimo bandymai\n",
    "\n",
    "new1 = np.array([1.,2.,3.])\n",
    "new1 = new1.reshape(1,3)\n",
    "print(new1, new1.shape)\n",
    "new2 = np.array([4.,5.,7.])\n",
    "new2 = new2.reshape(1,3)\n",
    "print(new2, new2.shape)\n",
    "new3 = np.array([6.,7.,8.])\n",
    "new3 = new3.reshape(1,3)\n",
    "print(new3, new3.shape)\n",
    "new4 = np.array([4.,5.,7.])\n",
    "new4 = new4.reshape(1,3)\n",
    "print(new4, new2.shape)\n",
    "# print(np.vstack((new1,new2)))\n",
    "# new = np.vstack((new1,new2))\n",
    "# print(new)\n",
    "\n",
    "# bandymas\n",
    "np_features_set = np.empty((0,3),float)\n",
    "# np_features_set = np.array([])\n",
    "print(np_features_set, np_features_set.shape)\n",
    "\n",
    "# np_features_set = np.vstack((np_features_set, new1))\n",
    "np_features_set= np.append(np_features_set, new1, axis=0)\n",
    "print(np_features_set, np_features_set.shape)\n",
    "\n",
    "# np_features_set= np.vstack(np_features_set, new2)\n",
    "np_features_set= np.append(np_features_set, new2, axis=0)\n",
    "print(np_features_set, np_features_set.shape)\n",
    "\n",
    "np_features_set= np.append(np_features_set, new3, axis=0)\n",
    "print(np_features_set, np_features_set.shape)\n",
    "\n",
    "np_features_set= np.append(np_features_set, new4, axis=0)\n",
    "print(np_features_set, np_features_set.shape)\n",
    "\n",
    "MEAN = np.mean(np_features_set, axis=0)\n",
    "STD = np.std(np_features_set, axis=0)\n",
    "# print(f'MEAN: {MEAN}')\n",
    "print(f'MEAN: {MEAN}  STD: {STD}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f36dab35816871602f0a4fffa6415a4e758bca001397bb3d9f7e90aab6637a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
