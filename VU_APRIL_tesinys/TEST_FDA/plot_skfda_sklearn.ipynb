{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scikit-fda and scikit-learn\n\nIn this section, we will explain how scikit-fda interacts  with the popular\nmachine learning package scikit-learn. We will introduce briefly the main\nconcepts of scikit-learn and how scikit-fda reuses the same concepts extending\nthem to the :term:`functional data analysis` field.\n\n.. Disable isort\n    isort:skip_file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Carlos Ramos Carre\u00f1o\n# License: MIT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A brief summary of scikit-learn architecture\n\nThe library [scikit-learn](https://scikit-learn.org/) is probably the most\nwell-known Python package for machine learning. This package focuses in\nmachine learning using multivariate data, which should be stored in a numpy\n:class:`~numpy.ndarray` in order to process it. However, this library has\ndefined a particular architecture that can be followed in order to provide\nnew tools that work in situations not even imagined by the original authors,\nwhile remaining compatible with the tools already provided in scikit-learn.\n\nIn scikit-fda, the same architecture is applied in order to work with\nfunctional data observations. As a result, scikit-fda tools are\nlargely compatible with scikit-learn tools, and it is possible to reuse\nobjects such as :class:`pipelines <sklearn.pipeline.Pipeline>` or even\nhyperparameter selection methods such as\n:class:`grid search cross-validation <sklearn.model_selection.GridSearchCV>`\nin the functional data setting.\n\nWe will introduce briefly the main concepts in scikit-learn, and explain how\nthe tools in scikit-fda are related with them. This is not intended as a full\nexplanation of scikit-learn architecture, and the reader is encouraged to\nlook at the [scikit-learn tutorials](https://scikit-learn.org/stable/tutorial/index.html) in order to achieve\na deeper understanding of it.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Estimator object\n\nA central concept in scikit-learn (and scikit-fda) is what is called an\nestimator. An estimator in this context is an object that can learn from\nthe data. Thus, classification, regression and clustering methods, as well\nas transformations with parameters learned from the training data are\nparticular kinds of estimators. Estimators can also be instanced passing\nparameters, which can be tuned to the data using hyperparameter selection\nmethods.\n\nEstimator objects have a ``fit`` method, with receive the training data\nand (if necessary) the training targets. This method uses the training data\nin order to learn some parameters of a model. When the learned parameters\nare part of the user-facing API, then by convention they are attributes of\nthe estimator ending in with the ``_`` character.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a concrete example of this, consider a nearest centroid classifier\nfor functional data. The object\n:class:`~skfda.ml.classification.NearestCentroid` is a classifier, and\nthus an estimator. As part of the training process the centroids of\nthe classes are computed and available as the learned parameter\n``centroids_``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The function :func:`~sklearn.model_selection.train_test_split` is\n    one of the functions originally from scikit-learn that can be\n    directly reused in scikit-fda.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skfda\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nX, y = skfda.datasets.fetch_growth(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclassifier = skfda.ml.classification.NearestCentroid()\nclassifier.fit(X_train, y_train)\nclassifier.centroids_.plot()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformers\n\n:term:`Transformers <sklearn:transformers>` are estimators which can convert\ndata to a new form. Examples of them are preprocessing methods, such as\nsmoothing, registration and dimensionality reduction methods. They always\nimplement ``fit_transform`` for fitting and transforming the data in one\nstep. The transformers may be :term:`sklearn:inductive`, which means that\ncan transform new data using the learned parameters. In that case they\nimplement the ``transform`` method to transform new data. If the\ntransformation is reversible, they usually also implement\n``\u00ecnverse_transform``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example consider the smoothing method\n:class:`skfda.preprocessing.smoothing.NadarayaWatsonHatMatrix`. Smoothing\nmethods attempt to remove noise from the data leveraging its continuous\nnature.\nAs these methods discard information of the original data they usually are\nnot reversible.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skfda.preprocessing.smoothing as ks\nfrom skfda.misc.hat_matrix import NadarayaWatsonHatMatrix\nX, y = skfda.datasets.fetch_phoneme(return_X_y=True)\n\n# Keep the first 5 functions\nX = X[:5]\n\nX.plot()\n\nsmoother = ks.KernelSmoother(kernel_estimator=NadarayaWatsonHatMatrix())\nX_smooth = smoother.fit_transform(X)\n\nX_smooth.plot()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predictors (classifiers, regressors, clusterers...)\n\n:term:`Predictors <sklearn:predictor>` in scikit-learn are estimators that\ncan assign a certain target to a particular observation. This includes\nsupervised methods such as classifiers (for which the target will be a class\nlabel), or regressors (for which the target is a real value, a vector, or,\nin functional data analysis, even a function!) and also unsupervised methods\nsuch as clusterers or outlying detector methods.\n\nPredictors should implement the ``fit_predict`` method for fitting the\nestimators and predicting the targets in one step and/or the ``predict``\nmethod for predicting the targets of possibly non previously observed data.\nUsually :term:`sklearn:transductive` estimators implement only the former\none, while :term:`sklearn:inductive` estimators implement the latter one (or\nboth).\n\nPredictors can have additional non-mandatory methods, such as\n``predict-proba`` for obtaining the probability of a particular prediction\nor ``score`` for evaluating the results of the prediction.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, we can look at the :class:`~skfda.ml.clustering.KMeans`\nclustering method for functional data. This method will try to separate\nthe data into different clusters according to the distance between\nobservations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skfda.datasets.fetch_weather(return_X_y=True)\n\n# Use only the first value (temperature)\nX = X.coordinates[0]\n\nclusterer = skfda.ml.clustering.KMeans(n_clusters=3)\ny_pred = clusterer.fit_predict(X)\n\nX.plot(group=y_pred)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metaestimators\n\nIn scikit-learn jargon, a :term:`sklearn:metaestimator` is an estimator\nthat takes other estimators as parameters. There are several reasons for\ndoing that, which will be explained now.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Composition metaestimators\n\nIt is very common in machine learning to apply one or more preprocessing\nsteps one after the other, before applying a final predictor. For this\npurpose scikit-learn offers the :class:`~sklearn.pipeline.Pipeline`, which\njoin the steps together and uses the same estimator API for performing all\nsteps in order (this is usually referred as the composite pattern in\nsoftware engineering). The :class:`~sklearn.pipeline.Pipeline` estimator\ncan be used with the functional data estimators available in scikit-fda.\nMoreover, as transformers such as dimensionality reduction methods can\nconvert functional data to multivariate data usable by scikit-learn methods\nit is possible to mix methods from scikit-fda and scikit-learn in the same\npipeline.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>In addition, scikit-learn offers estimators that can join several\n    transformations as new features of the same dataset (\n    :class:`~sklearn.pipeline.FeatureUnion`) or that can apply different\n    transformers to different columns of the data\n    (:class:`~sklearn.compose.ColumnTransformer`). These transformers\n    are not yet usable with functional data.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, we can construct a pipeline that registers the data using\nshift registation, then applies a variable selection method to\ntransform each observation to a 3D vector and then uses a SVM classifier\nto classify the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skfda.preprocessing.dim_reduction import variable_selection as vs\nfrom skfda.preprocessing.registration import LeastSquaresShiftRegistration\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\nX, y = skfda.datasets.fetch_growth(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\npipeline = Pipeline([\n    (\"registration\", LeastSquaresShiftRegistration()),\n    (\"dim_reduction\", vs.RKHSVariableSelection(n_features_to_select=3)),\n    (\"classifier\", SVC()),\n])\n\npipeline.fit(X_train, y_train)\npipeline.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter optimizers\n\nSome of the parameters used for the creation of an estimator need to be\ntuned to each particular dataset in order to improve the prediction accuracy\nand generalization. There are several techniques to do that already\navailable in scikit-learn, such as grid search cross-validation\n(:class:`~sklearn.model_selection.GridSearchCV`) or randomized search\n(:class:`~sklearn.model_selection.RandomizedSearchCV`). As these\nhyperparameter optimizers only need to split the data and call ``score`` in\nthe predictor, they can be directly used with the methods in scikit-fda.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>In addition one could use any optimizer that understand the scikit-learn\n    API such as those in [scikit-optimize](https://scikit-optimize.github.io).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, we will use :class:`~sklearn.model_selection.GridSearchCV`\nto select the number of neighbors used in a\n:class:`~skfda.ml.classification.KNeighborsClassifier`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n\nX, y = skfda.datasets.fetch_growth(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclassifier = skfda.ml.classification.KNeighborsClassifier()\n\ngrid_search = GridSearchCV(\n    estimator=classifier,\n    param_grid={\"n_neighbors\": range(1, 10, 2)},\n)\n\ngrid_search.fit(X_train, y_train)\nn_neighbors = grid_search.best_estimator_.n_neighbors\nscore = grid_search.score(X_test, y_test)\n\nprint(n_neighbors, score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Ensemble methods\n\nThe ensemble methods :class:`~sklearn.ensemble.VotingClassifier` and\n:class:`~sklearn.ensemble.VotingRegressor` in scikit-learn use several\ndifferent estimators in order to predict the targets. As this is done\nby evaluating the passed estimators as black boxes, these predictors can\nalso be combined with scikit-fda predictors.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>Other ensemble methods, such as\n    :class:`~sklearn.ensemble.BaggingClassifier` or\n    :class:`~sklearn.ensemble.AdaBoostClassifier` cannot yet\n    be used with functional data unless it has been\n    transformed to a multivariate dataset.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example we will use a voting classifier to classify data using as\nclassifiers a knn-classifier, a nearest centroid classifier and a\nmaximum depth classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n\nX, y = skfda.datasets.fetch_growth(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nknn = skfda.ml.classification.KNeighborsClassifier()\nnearest_centroid = skfda.ml.classification.NearestCentroid()\nmdc = skfda.ml.classification.MaximumDepthClassifier()\n\nvoting = VotingClassifier([\n    (\"knn\", knn),\n    (\"nearest_centroid\", nearest_centroid),\n    (\"mdc\", mdc),\n])\n\nvoting.fit(X_train, y_train)\nvoting.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Multiclass and multioutput classification utilities\n\nThe scikit-learn library also offers additional utilities that can convert\na binary classifier into a multiclass classifier (such as\n:class:`~sklearn.multiclass.OneVsRestClassifier`) or to extend a single\noutput classifier or regressor to accept also multioutput (vector-valued)\ntargets.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we want to use as a classifier the combination of a\ndimensionality reduction method (\n:class:`~skfda.preprocessing.dim_reduction.variable_selection.RKHSVariableSelection`)\nand a SVM classifier (:class:`~sklearn.svm.SVC`). As that particular\ndimensionality reduction method is only suitable for binary data, we use\n:class:`~sklearn.multiclass.OneVsRestClassifier` to classify in a\nmulticlass dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n\nX, y = skfda.datasets.fetch_phoneme(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\npipeline = Pipeline([\n    (\"dim_reduction\", vs.RKHSVariableSelection(n_features_to_select=3)),\n    (\"classifier\", SVC()),\n])\n\nmulticlass = OneVsRestClassifier(pipeline)\n\nmulticlass.fit(X_train, y_train)\nmulticlass.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Other scikit-learn utilities\n\nIn addition to the aforementioned objects, there are plenty of objects in\nscikit-learn that can be applied directly to functional data. We have\nalready seen in the examples the function\n:func:`~sklearn.model_selection.train_test_split`. Other objects and\nfunctions such as :class:`~sklearn.model_selection.KFold` can be directly\napplied to functional data in order to split it into folds. Scorers for\nclassification or regression, such as\n:func:`~sklearn.metrics.accuracy_score` can be directly applied to\nfunctional data problems.\n\nMoreover, there are plenty of libraries that aim to extend scikit-learn in\nseveral directions (take a look at the [list of related projects](https://scikit-learn.org/stable/related_projects.html)). You will\nprobably see that a lot of the functionality can be applied to scikit-fda,\nas it uses the same API as scikit-learn.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}