{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skriptas testiniam ECG sekų skaitymui ir požymių skaičiavimui.\n",
    "# Pritaikytas tiek MIT2ZIVE, tiek Zive duomenims\n",
    "\n",
    "# Perdarytas iš ReadSeqEKG_2022_10_05_v2.py, tam kad požymių skaičiavimo modulis\n",
    "# dirbtų su vienu pūpsniu. \n",
    "# \n",
    "# Originalų skriptą atsiuntė Povilas 2022 10 05, skriptas perdarytas.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys, json\n",
    "import math\n",
    "# import random\n",
    "\n",
    "import scipy.signal\n",
    "# import skfda\n",
    "from skfda import FDataGrid\n",
    "# from skfda.preprocessing.smoothing import BasisSmoother\n",
    "# from skfda.representation.basis import BSpline, Fourier\n",
    "\n",
    "def read_RR_arr_from_signal(atr_sample, idx, nl_steps, nr_steps):\n",
    "# Nuskaito ir pateikia EKG seką apie R dantelį seq: reikšmiu kiekis wl_side - iš kairės pusės, \n",
    "# reikšmiu kiekis wr_side - iš dešinės pusės, R dantelio vietą EKG įraše sample,\n",
    "# ir atitinkamo pūpsnio klasės numerį label: 0, 1, 2.\n",
    "# Taip pat pateikia seką RRl_arr iš nl_steps RR reikšmių tarp iš eilės einančių R dantelių į kairę nuo einamo R dantelio \n",
    "# ir seką RRr_arr nr_steps RR reikšmių tarp iš eilės einančių R dantelių į dešinę nuo einamo R dantelio dantelio.\n",
    "# Seka iš kairės RRl_arr prasideda nuo tolimiausio nuo R dantelio atskaitymo ir jai pasibaigus,\n",
    "# toliau ją pratesia RRl_arr. \n",
    "\n",
    "# **************************** Tikrinimai ******************************************************************\n",
    "\n",
    "    # Tikriname, ar skaičiuodami RR neišeisime už atr_sample ribų\n",
    "    if (idx + nr_steps) >= len(atr_sample):\n",
    "        txt = f\"Klaida 1! idx, nl_steps: {idx}, {nr_steps} Skaičiuojant RR viršijama pūpsnių atributo masyvo riba.\" \n",
    "        raise Exception(txt)  \n",
    "        # Reikia mažinti nr_steps arba koreguoti viršutinę idx ribą\n",
    "    \n",
    "    if ((idx - nl_steps) < 0):\n",
    "        txt = f\"Klaida 2! idx, nl_steps: {idx}, {nl_steps} Skaičiuojant RR išeinama už pūpsnių atributo masyvo ribų.\"\n",
    "        raise Exception(txt)  \n",
    "        # Reikia mažinti nl_steps arba didinti apatinę idx ribą \n",
    "    \n",
    "# **************************** Tikrinimų pabaiga ******************************************************************\n",
    "\n",
    "    # Suformuojame RR sekas kairėje ir dešinėje idx atžvilgiu\n",
    "    if (nl_steps != 0):\n",
    "        RRl_arr = np.zeros(shape=(nl_steps), dtype=int)\n",
    "        for i in range(nl_steps):\n",
    "            RRl_arr[nl_steps-i-1] = atr_sample[idx-i] - atr_sample[idx-i-1]\n",
    "    else:    \n",
    "        RRl_arr = None\n",
    "\n",
    "    if (nr_steps != 0):\n",
    "        RRr_arr = np.zeros(shape=(nr_steps), dtype=int)\n",
    "        for i in range(nr_steps):\n",
    "            RRr_arr[i] = atr_sample[idx+i+1] - atr_sample[idx+i]\n",
    "    else:\n",
    "        RRr_arr = None        \n",
    "    \n",
    "    return RRl_arr, RRr_arr\n",
    "\n",
    "def read_seq_from_signal(signal, atr_sample, idx, wl_side, wr_side):\n",
    "# Nuskaito ir pateikia EKG seką apie R dantelį seq: reikšmiu kiekis wl_side - iš kairės pusės, \n",
    "# reikšmiu kiekis wr_side - iš dešinės pusės, R dantelio vietą EKG įraše sample,\n",
    "\n",
    "    signal_length = signal.shape[0]\n",
    "    (seq_start, seq_end)  = get_seq_start_end(signal_length, atr_sample[idx], wl_side, wr_side)\n",
    "\n",
    "    # Tikriname, ar sekos langas neišeina už įrašo ribų\n",
    "    if (seq_start == None or seq_end == None):\n",
    "        raise Exception(f\"Klaida! {idx}: Sekos lango rėžiai už EKG įrašo ribų.\") \n",
    "        # Reikia mažinti wl_side ar wr_side, arba koreguoti idx ribas \n",
    "    else:    \n",
    "        seq = signal[seq_start:seq_end]\n",
    "\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_seq_start_end(signal_length, i_sample, window_left_side, window_right_side):\n",
    "    # Nustatome išskiriamos EKG sekos pradžią ir pabaigą\n",
    "    seq_start = i_sample - window_left_side\n",
    "    seq_end = i_sample + window_right_side\n",
    "    if (seq_start < 0 or seq_end > signal_length):\n",
    "        return (None, None)\n",
    "    else:\n",
    "        return (seq_start, seq_end)\n",
    "\n",
    "def read_rec(rec_dir, SubjCode):\n",
    "    file_path = Path(rec_dir, str(SubjCode) + '.npy')\n",
    "    signal = np.load(file_path, mmap_mode='r')\n",
    "    # print(f\"SubjCode: {SubjCode}  signal.shape: {signal.shape}\")\n",
    "    return signal\n",
    "\n",
    "def read_df_rpeaks(rec_dir, SubjCode):\n",
    "    # Pritaikyta nuskaityti json informaciją tiek mit2zive, tiek zive atvejams\n",
    "    file_path = Path(rec_dir, str(SubjCode) + '.json')\n",
    "\n",
    "    if (SubjCode > 1000): # zive atvejis\n",
    "        with open(file_path,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "            data = json.loads(f.read())\n",
    "        df = pd.json_normalize(data, record_path =['rpeaks'])\n",
    "    else: # mit2zive atvejis\n",
    "        df = pd.read_json(file_path, orient = 'records')\n",
    "    return df\n",
    "\n",
    "def split_SubjCode(SubjCode):\n",
    "    \"\"\"\n",
    "    Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "    \n",
    "    zive atveju: SubjCode = int(str(userNr) + str(registrationNr)), kur userNr >= 1000,\n",
    "    pvz. SubjCode = 10001\n",
    "    mit2zive atveju: SubjCode = userNr,  kur userNr < 1000,\n",
    "    pvz. SubjCode = 101\n",
    "    https://www.adamsmith.haus/python/answers/how-to-get-the-part-of-a-string-before-a-specific-character-in-python\n",
    "    Parameters\n",
    "    ------------\n",
    "        SubjCode: int\n",
    "    Return\n",
    "    -----------\n",
    "        userNr: int\n",
    "        recordingNr: int\n",
    "    \"\"\"   \n",
    "    if (SubjCode < 1000):\n",
    "        userNr = SubjCode\n",
    "        recordingNr = 0   \n",
    "        return userNr, recordingNr\n",
    "    else:        \n",
    "        str_code = str(SubjCode) \n",
    "        chars = list(str_code)\n",
    "        str1 =\"\"\n",
    "        userNr = int(str1.join(chars[:4]))\n",
    "        str2 =\"\"\n",
    "        recordingNr = int(str2.join(chars[4:]))\n",
    "        return userNr, recordingNr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spike_width(orig, derivate, reample_points, positions):\n",
    "    ret = pd.DataFrame(columns=[\"P_val\", \"Q_val\", \"R_val\", \"S_val\", \"T_val\", \"P_pos\", \"Q_pos\", \"R_pos\", \"S_pos\", \"T_pos\", \"QRS\", \"PR\",\"ST\",\"QT\"])\n",
    "    R = positions[0]\n",
    "    asign = np.sign(derivate)\n",
    "    signchange = ((np.roll(asign, 1) - asign) != 0).astype(int)\n",
    "    Q = None\n",
    "    for down in range(positions[0] - 1, 0, -1):\n",
    "        if signchange[down] == 1:\n",
    "            Q = down\n",
    "            break\n",
    "    S = None\n",
    "    times_changed = 0\n",
    "    for up in range(positions[0], reample_points, 1):\n",
    "        if (signchange[up] == 1):\n",
    "            if (times_changed == 1):\n",
    "                S = up\n",
    "                break\n",
    "            else:\n",
    "                times_changed += 1\n",
    "    if (Q != None) & (S != None):\n",
    "        QRS = math.fabs(S-Q)\n",
    "        P=positions[1]\n",
    "        T=positions[2]\n",
    "        PR = math.fabs(Q-P)\n",
    "        ST = math.fabs(T-S)\n",
    "        QT = math.fabs(T-Q)\n",
    "        row = {\"P_val\": orig[P], \"Q_val\":orig[Q], \"R_val\": orig[R], \"S_val\": orig[S], \"T_val\":orig[T],\n",
    "                           \"P_pos\":P * 1./ reample_points, \"Q_pos\":Q * 1./ reample_points,\n",
    "                           \"R_pos\":R * 1./ reample_points, \"S_pos\":S * 1./ reample_points,\n",
    "                           \"T_pos\": T * 1./ reample_points,\n",
    "                           \"QRS\":QRS * 1./ reample_points, \"PR\":PR * 1./ reample_points,\n",
    "                           \"ST\":ST * 1./ reample_points, \"QT\":QT * 1./ reample_points}\n",
    "        ret = pd.DataFrame(row, index=[0])\n",
    "        return ret\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_beat_features_fda(signal, atr_sample, idx):\n",
    "    resapmling_points = 200\n",
    "    fraction_to_drop_l = 0.7\n",
    "    fraction_to_drop_r = 0.7\n",
    "    samples = np.linspace(0, 1, resapmling_points)\n",
    "    nl_RR =1\n",
    "    nr_RR = 1\n",
    "    keys_RR = []\n",
    "    for tmp_l in range(nl_RR):\n",
    "        keys_RR.append('RR_l_' + str(tmp_l))\n",
    "    for tmp_l in range(nl_RR - 1):\n",
    "        keys_RR.append('RR_l_' + str(tmp_l) + '/' + 'RR_l_' + str(tmp_l + 1))\n",
    "    for tmp_r in range(nr_RR):\n",
    "        keys_RR.append('RR_r_' + str(tmp_r))\n",
    "    for tmp_r in range(nr_RR):\n",
    "        keys_RR.append('RR_r_' + str(tmp_r) + '/' + 'RR_r_' + str(tmp_r + 1))\n",
    "\n",
    "    train_set_stats = pd.DataFrame()\n",
    "    train_set_points = pd.DataFrame()\n",
    "    Rythm_Data = pd.DataFrame()\n",
    "    omit_idx = pd.DataFrame()\n",
    "\n",
    "    RRl_arr, RRr_arr = read_RR_arr_from_signal(atr_sample, idx, nl_steps=1, nr_steps=1)\n",
    "    wl_side = math.floor(RRl_arr[0] * fraction_to_drop_l)  # pakeista all_beats_attr.loc[idx][5] į RRl, kj\n",
    "    wr_side = math.floor(RRr_arr[0] * fraction_to_drop_r)  # pakeista all_beats_attr.loc[idx][6] į RRr, kj\n",
    "    \n",
    "    seq_1d = read_seq_from_signal(signal, atr_sample, idx, wl_side, wr_side)\n",
    "\n",
    "    RPT = []\n",
    "    dictRR = {}\n",
    "    for tmp_l in range(nl_RR):\n",
    "        dictRR[keys_RR[tmp_l]] = RRl_arr[tmp_l]\n",
    "    for tmp_l in range(nl_RR - 1):\n",
    "        dictRR[keys_RR[tmp_l + nl_RR]] = RRl_arr[tmp_l] / RRl_arr[tmp_l + 1]\n",
    "        #keys_RR.append('RR_l_' + str(tmp_l) + '/' + 'RR_l_' + str(tmp_l + 1))\n",
    "    for tmp_r in range(nr_RR):\n",
    "        dictRR[keys_RR[tmp_r + 2 * nl_RR - 1]] = RRr_arr[tmp_r]\n",
    "    for tmp_r in range(nr_RR - 1):\n",
    "        dictRR[keys_RR[tmp_r + 3 * nl_RR - 1]] = RRr_arr[tmp_r] / RRr_arr[tmp_r + 1]\n",
    "    dictRR['RR_r/RR_l'] = RRl_arr[tmp_r] / RRr_arr[tmp_l]\n",
    "    \n",
    "    fd = FDataGrid(data_matrix=seq_1d)\n",
    "\n",
    "    fdd = fd.derivative()\n",
    "    fd = fd.evaluate(samples)\n",
    "    fd = fd.reshape(resapmling_points)\n",
    "\n",
    "    fdd = fdd.evaluate(samples)\n",
    "    fdd = fdd.reshape(resapmling_points)\n",
    "\n",
    "    RPT.append(fd.argmax()) #0-th  element is R\n",
    "\n",
    "    indexes_lower = scipy.signal.argrelextrema(fdd[math.floor(RPT[0]*0.5):RPT[0] - math.floor(RPT[0]*0.05)], comparator=np.greater, order=3)\n",
    "    indexes_lower = tuple([math.floor(RPT[0]*0.5) + 1 + x for x in indexes_lower])\n",
    "    values_lower = fd[indexes_lower]\n",
    "    ind_low = np.argpartition(values_lower, -1)[-1:]\n",
    "    ind_low[::-1].sort()\n",
    "\n",
    "    indexes_upper = scipy.signal.argrelextrema(fdd[RPT[0] + 2:math.floor(RPT[0]*1.8)], comparator=np.greater, order=3) + RPT[0] + 2\n",
    "    values_upper = fd[indexes_upper[0]]\n",
    "    ind_up = np.argpartition(values_upper, -1)[-1:]\n",
    "    tmp1 = np.sort(ind_up)\n",
    "\n",
    "    if (len(ind_low)>=1) & (len(ind_up)>=1):\n",
    "\n",
    "# I. Rythm_Data: 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val', 'P_pos', 'Q_pos', 'R_pos',\n",
    "#  'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT' - viso 14 požymių\n",
    "\n",
    "        RPT.append(indexes_lower[0][ind_low[0]])  # 1-st is P\n",
    "        RPT.append(indexes_upper[0][tmp1[0]]) # 2nd is T\n",
    "    \n",
    "        Rythm_Data = get_spike_width(fd, fdd, resapmling_points, RPT)\n",
    "        if not Rythm_Data.empty:\n",
    "\n",
    "# II. train_set_stats\": 'idx', 'seq_size', 'RR_l_0', 'RR_r_0', 'RR_r/RR_l', 'wl_side', 'wr_side',\n",
    "# 'signal_mean', 'signal_std' - viso 9 požymiai\n",
    "\n",
    "            dict_full ={'idx': idx, 'seq_size': seq_1d.shape[0]}\n",
    "            dict_full = dict(dict_full, **dictRR)\n",
    "            dict_full.update({'wl_side': wl_side, 'wr_side': wr_side, \"signal_mean\": np.mean(fd), \"signal_std\": np.std(fd)})\n",
    "            # print(f'dict: {dict_full}')\n",
    "            train_set_stats = pd.DataFrame(dict_full, index=[0])\n",
    "\n",
    "# III. train_set_points: '0', '1', '2', ... , '197', '198', '199' - viso 200 požymių\n",
    "            \n",
    "            train_set_points = pd.Series(fd).to_frame().T\n",
    "        else:\n",
    "            omit_idx = pd.DataFrame({'idx': idx}, index=[0])\n",
    "    else:\n",
    "        omit_idx = pd.DataFrame({'idx': idx}, index=[0])\n",
    "\n",
    "    train_set_stats = pd.concat([train_set_stats, Rythm_Data, train_set_points], axis=1)\n",
    "    return train_set_stats, omit_idx\n",
    "\n",
    "\n",
    "def get_beat_features_fda_set(signal, df_rpeaks, idx_lst):\n",
    "# Apskaičiuojami užduotų EKG signalo pūpsnių (per idx_lst) požymiai ir iš jų\n",
    "# suformuojamas požymių dataframe masyvas, pridedant 'label'\n",
    "\n",
    "    # all_beats pritaikytas tiek MIT2ZIVE, tiek ZIVE duomenims\n",
    "    all_beats = {'N':0,'R':0, 'L':0, 'e':0, 'j':0, 'A':1,'a':1, 'J':1, 'S':1, 'V':2, 'E':2, 'F':3, 'U':3, 'Q':3}\n",
    "\n",
    "    beat_features_set = pd.DataFrame()\n",
    "    omit_idx_set = pd.DataFrame()\n",
    "\n",
    "    atr_sample = df_rpeaks['sampleIndex'].to_numpy()\n",
    "    atr_symbol = df_rpeaks['annotationValue'].to_numpy()\n",
    "    # Jei pasitaiko symbol 'U' arba 'F', pūpsniui suteikiame klasę 3, kurią vėliau apvalysime  \n",
    "    test_labels = np.array([all_beats[symbol] for symbol in atr_symbol])\n",
    "    \n",
    "    # print(\"\\nGet beat features set from signal:\")\n",
    "    for idx in idx_lst:\n",
    "        beat_features, omit_idx = get_beat_features_fda(signal, atr_sample, idx)\n",
    "        # beat_features_set = beat_features_set.append(beat_features, ignore_index=True)\n",
    "        if omit_idx.empty:\n",
    "            beat_features['label'] = test_labels[idx]\n",
    "            beat_features_set = pd.concat([beat_features_set, beat_features])\n",
    "        else:\n",
    "            omit_idx_set = pd.concat([omit_idx_set, omit_idx])\n",
    "    \n",
    "    \n",
    "    # Konvertuojame int pozymius į float64\n",
    "    beat_features_set['RR_l_0'] = beat_features_set['RR_l_0'].astype(float)\n",
    "    beat_features_set['RR_r_0'] = beat_features_set['RR_r_0'].astype(float)\n",
    "\n",
    "    return beat_features_set, omit_idx_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\DI\\ZIVEO_2022_RUDUO\\VU_APRIL_tesinys\\PVZ_SKIRTAS_VU_RUDUO\n",
      "[100, 10021]\n",
      "\n",
      "SubjCode: 100 userNr: 100  signal_length: 361111\n",
      "[2, 3, 4, 5]\n",
      "\n",
      "train_set_data:\n",
      "     seq_size  RR_l_0  RR_r_0  RR_r/RR_l  wl_side  wr_side  signal_mean  signal_std     P_val     Q_val     R_val     S_val     T_val  P_pos  Q_pos  R_pos  S_pos  T_pos    QRS     PR     ST     QT         0         1         2         3         4         5       199\n",
      "idx                                                                                                                                                                                                                                                                       \n",
      "2         227   163.0   162.0   1.006173      114      113    -0.314849    0.155551 -0.210547 -0.532434  0.901030 -0.437992 -0.306871  0.350  0.480  0.505  0.530  0.850  0.050  0.130  0.320  0.370 -0.345000 -0.346172 -0.360271 -0.360593 -0.371940 -0.369789 -0.343998\n",
      "3         223   162.0   158.0   1.025316      113      110    -0.340495    0.144071 -0.224874 -0.541396  0.843898 -0.453412 -0.319268  0.395  0.490  0.510  0.530  0.835  0.040  0.095  0.305  0.345 -0.427999 -0.432776 -0.450995 -0.443079 -0.452087 -0.459156 -0.383003\n",
      "4         220   158.0   158.0   1.000000      110      110    -0.337768    0.136691 -0.248173 -0.606742  0.811256 -0.430214 -0.288670  0.345  0.480  0.505  0.525  0.825  0.045  0.135  0.300  0.345 -0.415001 -0.419899 -0.417792 -0.419330 -0.434400 -0.427461 -0.345000\n",
      "5         220   158.0   158.0   1.000000      110      110    -0.333569    0.128838 -0.245452 -0.497076  0.770206 -0.434827 -0.297069  0.335  0.485  0.505  0.525  0.845  0.040  0.150  0.320  0.360 -0.383995 -0.370112 -0.379600 -0.375282 -0.371814 -0.384543 -0.340000\n",
      "\n",
      "omitted:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "SubjCode: 10021 userNr: 1002  signal_length: 127999\n",
      "[2, 3, 4, 5]\n",
      "\n",
      "train_set_data:\n",
      "     seq_size  RR_l_0  RR_r_0  RR_r/RR_l  wl_side  wr_side  signal_mean  signal_std     P_val     Q_val     R_val     S_val     T_val  P_pos  Q_pos  R_pos  S_pos  T_pos    QRS     PR     ST     QT         0         1         2         3         4         5       199\n",
      "idx                                                                                                                                                                                                                                                                       \n",
      "2         224   161.0   160.0   1.006250      112      112    -0.976929    0.249471 -1.051418 -1.126880  0.331089 -0.990498 -0.791692  0.395  0.475  0.500  0.550  0.700  0.075  0.080  0.150  0.225 -1.209105 -1.191686 -1.178183 -1.169022 -1.159616 -1.150088 -0.717963\n",
      "3         224   160.0   161.0   0.993789      112      112    -0.569963    0.203299 -0.633450 -0.726075  0.561081 -0.578219 -0.321010  0.440  0.475  0.500  0.555  0.725  0.080  0.035  0.170  0.250 -0.746062 -0.730959 -0.717548 -0.704471 -0.690818 -0.683725 -0.415001\n",
      "4         221   161.0   157.0   1.025478      112      109    -0.347749    0.147709 -0.380549 -0.498572  0.706259 -0.387505 -0.192859  0.380  0.480  0.505  0.560  0.720  0.080  0.100  0.160  0.240 -0.340410 -0.326603 -0.310519 -0.296295 -0.287289 -0.284490 -0.341091\n",
      "5         225   157.0   166.0   0.945783      109      116    -0.182672    0.181011 -0.278863 -0.390864  0.871680 -0.229046 -0.017541  0.380  0.460  0.485  0.510  0.685  0.050  0.080  0.175  0.225 -0.179988 -0.167165 -0.152338 -0.137267 -0.129658 -0.126720 -0.035575\n",
      "\n",
      "omitted:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Pabaiga\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", 6000)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    " # Išvedamų požymių sąrašas \n",
    "selected_features = ['seq_size','RR_l_0', 'RR_r_0', 'RR_r/RR_l', 'wl_side','wr_side',\n",
    "                'signal_mean', 'signal_std', 'P_val', 'Q_val', 'R_val', 'S_val', 'T_val',\n",
    "                'P_pos', 'Q_pos', 'R_pos', 'S_pos', 'T_pos', 'QRS', 'PR', 'ST', 'QT', '0', '1', '2',\n",
    "                '3', '4', '5', '199']\n",
    "\n",
    "db_path = Path.cwd()\n",
    "print(db_path)\n",
    "\n",
    "SubjCodes = [100, 10021]\n",
    "print(SubjCodes)\n",
    "\n",
    "for SubjCode in SubjCodes:\n",
    "    sign_raw = read_rec(db_path, SubjCode)\n",
    "    signal_length = sign_raw.shape[0]\n",
    "    signal = sign_raw\n",
    "\n",
    "    # Surandame ir išvedame įrašo atributus\n",
    "    userNr, recNr = split_SubjCode(SubjCode)\n",
    "    # print(f\"\\nSubjCode: {SubjCode} userNr: {userNr:>2} file_name: {file_name:>2} signal_length: {signal_length}\")\n",
    "    print(f\"\\nSubjCode: {SubjCode} userNr: {userNr:>2}  signal_length: {signal_length}\")\n",
    "\n",
    "    df_rpeaks = read_df_rpeaks(db_path, SubjCode)\n",
    "\n",
    "    # Užduodame pūpsnius, kuriems skaičiuosime požymius \n",
    "    idx_lst = [2, 3, 4, 5]\n",
    "    print(idx_lst)\n",
    "\n",
    "    train_set_data, omitted = get_beat_features_fda_set(signal, df_rpeaks, idx_lst)\n",
    "    data_frame = train_set_data.set_index('idx')\n",
    "    data_frame.columns = data_frame.columns.astype(str)\n",
    "\n",
    "    # paruošiame atrinktų požymių masyvą spausdinimui\n",
    "    data_frame_init = data_frame[selected_features]\n",
    "    print(\"\\ntrain_set_data:\")\n",
    "    print(data_frame_init.head())\n",
    "    print(\"\\nomitted:\")\n",
    "    print(omitted.head())\n",
    "\n",
    "print(\"\\nPabaiga\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1f36dab35816871602f0a4fffa6415a4e758bca001397bb3d9f7e90aab6637a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
