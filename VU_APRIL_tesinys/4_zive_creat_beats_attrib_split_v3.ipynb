{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skriptas pūpsnių atributų masyvui all_beats_attr sukurimui, o taip pat pacientų padalijimui\n",
    "# į dalis: train, validate, test, sukuria tų dalių SubjCode sąrašus ir įrašo juos į diską.\n",
    "#  \n",
    "# Atnaujintas variantas, po to, kaip padaryti pakeitimai failų varduose 2022 03 26\n",
    "#\n",
    "# Skriptas zive_creat_beats_attrib skirtas zive EKG įrašuose esančių pūpsnių atributų masyvui\n",
    "# all_beats_attr sukurti, kad būtų galima juos panaudoti sekų formavimui:\n",
    "# - atsikratome nepageidaujamų anotacijų: 'U'  ------- ///////////////////// pataisyti, neatsikratyti\n",
    "# - iš anotacijų suformuojame klasių numerius\n",
    "# - apskaičiuojami ir į atributus įrašomi RR intervalai: RRl, RRr. Atributų eilutėse,\n",
    "#  atitinkančios pirmą ir paskutinį pūpsnį, RRl ir RRr reikšmės lygios ???????????????????????????????????????\n",
    "#  sentinel = -1  - nereikia, skaičiuojama programiškai /////////////// ///////////////////////\n",
    "\n",
    "# Pacientų kodų sąrašas SubjCodes (userNr+file_name) paimamas iš list_npy.json, kurį\n",
    "# suformuoja zive_create_npy. Masyvas all_beats_attr įrašomas į failą all_beats_attr_z.csv.\n",
    "\n",
    "# Toliau pacientai sudalijami į dalis: train, validate, test. Šioms dalims sukūriami pacientų įrašų vardų SubjCode\n",
    "# sąrašai train_subjcode_lst, validate_subjcode_lst, test_subjcode_lst ir jie įrašomi į diską, kur paskui bus\n",
    "# naudojami indeksų generavimui.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from icecream import ic\n",
    "import json, time, sys\n",
    "\n",
    "from zive_util_vu import runtime, read_rec_attrib, split_SubjCode, get_annotations_table, print_annotations_table\n",
    "\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def print_df_list(df_list, flag1=True, flag2=True):\n",
    "    if flag1:\n",
    "        print(df_list[['SubjCode', 'file_name',  'N',   'S',    'V',   'U']])\n",
    "    if flag2:\n",
    "        sum = np.zeros(4,'int')\n",
    "        # print(' '*5, 'N',   'S',   'V',  'U')\n",
    "        sum[0] = df_list.iloc[:, 4].sum()\n",
    "        sum[1] = df_list.iloc[:, 5].sum()\n",
    "        sum[2] = df_list.iloc[:, 6].sum()\n",
    "        sum[3] = df_list.iloc[:, 7].sum()\n",
    "        total = sum.sum()\n",
    "        str =f\"\\nN:{int(sum[0]):>7}  S:{(int(sum[1])):5}  V:{int(sum[2]):5}  U:{int(sum[3]):4}  total:{total:>7}\" \n",
    "        print(str)\n",
    "\n",
    "my_os=sys.platform\n",
    "\n",
    "print(\"OS in my system : \",my_os)\n",
    "\n",
    "if my_os != 'linux':\n",
    "    OS = 'Windows'\n",
    "else:  \n",
    "    OS = 'Ubuntu'\n",
    "\n",
    "# Pasiruošimas\n",
    "\n",
    "# //////////////// NURODOMI PARAMETRAI /////////////////////////////////////////////////////\n",
    "\n",
    "# Bendras duomenų aplankas, kuriame patalpintas subfolderis name_db\n",
    "\n",
    "if OS == 'Windows':\n",
    "    Duomenu_aplankas = 'D:\\DI\\Data\\MIT&ZIVE\\VU'   # variantas: Windows\n",
    "else:\n",
    "    Duomenu_aplankas = '/home/kesju/DI/Data/MIT&ZIVE/VU'   # arba variantas: UBUNTU, be Docker\n",
    "\n",
    "# jei variantas Docker pasirenkame:\n",
    "# Duomenu_aplankas = '/Data/MIT&ZIVE'\n",
    "\n",
    "#  MIT2ZIVE duomenų aplankas\n",
    "db_folder = 'DUOM_VU'\n",
    "\n",
    "# Aplankas su MIT2ZIVE EKG įrašais (.npy) ir anotacijomis (.json)\n",
    "rec_folder = 'records_npy'\n",
    "rec_list_npy = 'list_npy.json'\n",
    "train_beats_attr_fname ='all_beats_attr_z.csv'\n",
    "\n",
    "# Kas kiek išvedamas apdorotų duomenų skaičius\n",
    "show_period = 100\n",
    "\n",
    "# Failai pūpsnių klasių formavimui\n",
    "annot_grouping = {'N':'N', 'S':'S', 'V':'V', 'U':'U'}\n",
    "selected_beats = {'N':0, 'S':1, 'V':2, 'U':3}\n",
    "\n",
    "# ///////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "# Nuoroda į MIT2ZIVE duomenų aplanką\n",
    "db_path = Path(Duomenu_aplankas, db_folder)\n",
    "\n",
    "# Nuoroda į aplanką su MIT2ZIVE EKG įrašais (.npa) ir anotacijomis (.json)\n",
    "rec_dir = Path(db_path, rec_folder)\n",
    "\n",
    "# Nuskaitome failą info_create.json ir duomenų rinkinio parametrus\n",
    "file_path = Path(rec_dir,'info_create_z.json')\n",
    "with open(file_path) as json_file:\n",
    "    info_create = json.load(json_file)\n",
    "\n",
    "fs = info_create['fs'] # diskretizavimo dažnumas\n",
    "SubjCodes =  info_create['SubjCodes'] # pacientų įrašų sąrašas\n",
    "annot_list  = info_create['annot_list'] # anotacijų sąrašas\n",
    "# print(annot_list)\n",
    "\n",
    "# Susikuriame pagalbinį anotacijų žodyną - dictionary beats_annot\n",
    "nr_sequence = list(range(13))\n",
    "beats_annot = dict(zip(annot_list, nr_sequence))\n",
    "\n",
    "print(\"\\nSkriptas skirtas zive EKG įrašuose esančių pūpsnių atributų sarašui all_beats_attr sukurti\")\n",
    "print(\"\\nBendras Zive duomenų aplankas: \", Duomenu_aplankas)\n",
    "print(\"Zive EKG įrašų aplankas: \", rec_dir)\n",
    "print(\"EKG įrašų atributų sąrašas:\", rec_list_npy)\n",
    "\n",
    "print(\"Diskretizavimo dažnis: \", fs)\n",
    "print(\"\\nPacientų įrašų kodų sąrašas:\\n\",SubjCodes)\n",
    "print(\"\\nAnotacijų sąrašas:\\n\", annot_list)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 19)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Nuskaitomas įrašų sąrašas, suformuojamas atitinkamas dataframe df_list\n",
    "file_path = Path(rec_dir, rec_list_npy)\n",
    "with open(file_path,'r', encoding='UTF-8', errors = 'ignore') as f:\n",
    "    data = json.loads(f.read())\n",
    "df_list = pd.json_normalize(data, record_path =['data'])\n",
    "print(\"\\ndf_list:\")\n",
    "print_df_list(df_list)\n",
    "\n",
    "# Susirandame anotacijų pasiskirstymą per įrašus\n",
    "annot_list = ['N', 'S', 'V', 'U']\n",
    "# Susikuriame pagalbinį anotacijų žodyną - dictionary beats_annot\n",
    "nr_sequence = list(range(len(annot_list)))\n",
    "beats_annot = dict(zip(annot_list, nr_sequence))\n",
    "# print(beats_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# II-a dalis. Formuojamas freimas all_beats_attr ir  įrašomas į diską. \n",
    "# Galima praleisti ir eiti tiesiai prie III-ios dalies\n",
    "\n",
    "print(\"\\nEKG įrašams suformuojamas pūpsnių atributų freimas ir įrašomas į diską\")\n",
    "\n",
    "# Pacientų įrašų sąrašas bandymams:\n",
    "# SubjCodes = ['10001']\n",
    "\n",
    "# Sukūriame masyvą sekų atributų sąrašo kaupimui\n",
    "all_beats_attr = pd.DataFrame({'userNr': pd.Series(dtype='int'),\n",
    "                   'recordingNr': pd.Series(dtype='int'),\n",
    "                   'sample': pd.Series(dtype='int'),\n",
    "                   'symbol': pd.Series(dtype='str')})\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# CIKLAS PER PACIENTŲ ĮRAŠUS\n",
    "\n",
    "# for SubjCode in SubjCodes:\n",
    "for idx, row in df_list.iterrows():    \n",
    "    SubjCode = row['SubjCode']\n",
    "    print(\"\\nECG įrašas:\", SubjCode)\n",
    "    \n",
    "    # Paciento anotacijų ir EKG įrašų nuskaitymas, sekų išpjovimas ir įrašymas, sekų atributų formavimas\n",
    "\n",
    "    # Nuskaitome paciento anotacijas ir jų indeksus\n",
    "    atr_sample, atr_symbol = read_rec_attrib(rec_dir, SubjCode)\n",
    "\n",
    "    subject_labels = []\n",
    "    beat_nr = 0\n",
    "    icycle = 0\n",
    "\n",
    "    # Ciklas per visas paciento įrašo anotacijas (simbolius) ir jų vietas (i_sample)\n",
    "    for i, i_sample in enumerate(atr_sample):\n",
    "    \n",
    "        icycle +=1\n",
    "        if (icycle%show_period == 0):\n",
    "            print(icycle, end =' ') \n",
    "\n",
    "        # Formuojame pūpsnio atributus\n",
    "        userNr, recordingNr = split_SubjCode(SubjCode)\n",
    "\n",
    "        beats_attr = {'userNr':int(userNr), 'recordingNr':int(recordingNr), 'sample':int(i_sample), 'symbol':str(atr_symbol[i])}\n",
    "        \n",
    "        # Senas variantas\n",
    "        # train_beats_attr = train_beats_attr.append(beats_attr, ignore_index=True)\n",
    "\n",
    "        # Variantas su concat\n",
    "        df_new_row = pd.DataFrame([beats_attr])\n",
    "        all_beats_attr = pd.concat([all_beats_attr, df_new_row])\n",
    "\n",
    "        # Galima perrašyti ir kitaip, naudojant pd.DataFrame.from_records\n",
    "        # all_beats_attr = pd.concat([all_beats_attr, pd.DataFrame.from_records([beats_attr])])\n",
    "\n",
    "        beat_nr +=1\n",
    "\n",
    "# Ciklo per pacientų įrašus pabaiga\n",
    "\n",
    "# Atsikratome nepageidaujamų anotacijų: 'U'\n",
    "# index_names = train_beats_attr[train_beats_attr['symbol'].isin(['U'])].index\n",
    "# train_beats_attr.drop(index_names, inplace = True)\n",
    "# print(train_beats_attr.info())\n",
    "\n",
    "# Pernumeruojame indeksus, kad būtų nuo 0 iš eilės\n",
    "all_beats_attr.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# Iš anotacijų suformuojame klasių numerius ir pridedame, kaip naują stulpelį\n",
    "annot_labels = {key:selected_beats[value] for key, value in annot_grouping.items()}\n",
    "labels_from_annot = all_beats_attr['symbol'].replace(annot_labels, inplace=False)\n",
    "all_beats_attr['label'] = labels_from_annot\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\nVisi duomenys\\n\")\n",
    "labels_table, labels_sums = get_annotations_table(all_beats_attr)\n",
    "print_annotations_table(labels_table, labels_sums, Flag1=True, Flag2=False)\n",
    "\n",
    "# Įrašome sekos atributų masyvą į seq_dir aplanką\n",
    "file_path = Path(rec_dir, train_beats_attr_fname)\n",
    "all_beats_attr.index.name = \"id\"\n",
    "\n",
    "all_beats_attr.to_csv(file_path)\n",
    "print(\"\\nAtributų freimas įrašytas: į \", file_path, \"\\n\" )\n",
    "\n",
    "end_time = time.time()\n",
    "print('\\n')\n",
    "runtime(end_time-start_time)\n",
    "\n",
    "print(\"\\nPabaiga.............\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  III-a dalis. SUDALIJAME PACIENTUS IR JŲ ĮRAŠUS Į TRAIN, VALIDATION IR TEST DALIS\n",
    "# Gausime df_train, df_validation, df_test, atitinkamus SubjCodes sąrašus įrašome į diską\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_stratified_into_train_val_test(df_input, stratify_colname='y',\n",
    "                                         frac_train=0.6, frac_val=0.15, frac_test=0.25,\n",
    "                                         random_state=None):\n",
    "    '''\n",
    "    https://localcoder.org/stratified-splitting-of-pandas-dataframe-into-training-validation-and-test-set\n",
    "\n",
    "    Splits a Pandas dataframe into three subsets (train, val, and test)\n",
    "    following fractional ratios provided by the user, where each subset is\n",
    "    stratified by the values in a specific column (that is, each subset has\n",
    "    the same relative frequency of the values in the column). It performs this\n",
    "    splitting by running train_test_split() twice.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_input : Pandas dataframe\n",
    "        Input dataframe to be split.\n",
    "    stratify_colname : str\n",
    "        The name of the column that will be used for stratification. Usually\n",
    "        this column would be for the label.\n",
    "    frac_train : float\n",
    "    frac_val   : float\n",
    "    frac_test  : float\n",
    "        The ratios with which the dataframe will be split into train, val, and\n",
    "        test data. The values should be expressed as float fractions and should\n",
    "        sum to 1.0.\n",
    "    random_state : int, None, or RandomStateInstance\n",
    "        Value to be passed to train_test_split().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_train, df_val, df_test :\n",
    "        Dataframes containing the three splits.\n",
    "    '''\n",
    "\n",
    "    if frac_train + frac_val + frac_test != 1.0:\n",
    "        raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
    "                         (frac_train, frac_val, frac_test))\n",
    "\n",
    "    if stratify_colname not in df_input.columns:\n",
    "        raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n",
    "\n",
    "    X = df_input # Contains all columns.\n",
    "    y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n",
    "\n",
    "    # Split original dataframe into train and temp dataframes.\n",
    "    df_train, df_temp, y_train, y_temp = train_test_split(X,\n",
    "                                                          y,\n",
    "                                                          stratify=y,\n",
    "                                                          test_size=(1.0 - frac_train),\n",
    "                                                          random_state=random_state)\n",
    "\n",
    "    # Split the temp dataframe into val and test dataframes.\n",
    "    relative_frac_test = frac_test / (frac_val + frac_test)\n",
    "    df_val, df_test, y_val, y_test = train_test_split(df_temp,\n",
    "                                                      y_temp,\n",
    "                                                      stratify=y_temp,\n",
    "                                                      test_size=relative_frac_test,\n",
    "                                                      random_state=random_state)\n",
    "\n",
    "    assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n",
    "\n",
    "    return df_train, df_val, df_test\n",
    "\n",
    "\n",
    "print(\"\\nIII-a dalis. SUDALIJAME PACIENTUS IR JŲ ĮRAŠUS Į TRAIN, VALIDATION IR TEST DALIS\")\n",
    "\n",
    "# Surenkame informaciją apie pacientus\n",
    "\n",
    "df_sum = df_list.groupby(['userId'],sort = False).sum()\n",
    "# print(df_sum)\n",
    "# https://sparkbyexamples.com/pandas/pandas-groupby-sum-examples/\n",
    "count = df_list['userId'].value_counts()\n",
    "print(f'\\nViso pacientų: {len(count)}  EKG įrašų: {len(df_list)}')\n",
    "print(f'\\nĮrašų pasiskirstymas per pacientus')\n",
    "count = count.rename(\"count\")\n",
    "frames = [df_sum, count]\n",
    "result = pd.concat(frames, axis = 1)\n",
    "result.index.rename ('userId', inplace= True)\n",
    "# https://www.shanelynn.ie/pandas-drop-delete-dataframe-rows-columns/\n",
    "\n",
    "# print(result[['N',   'S',    'V',   'U', 'count']])\n",
    "print_df_list(df_list, flag1=False, flag2=True)\n",
    "\n",
    "# Išplečiame freimą result iki pozymio user_type, kurį naudosime kaip stratify splitinant į train, test\n",
    "\n",
    "# Nustatome user_type pagal bendrus 'S' ir 'V' skaičius per visus paciento įrašus \n",
    "# user_type == 0, jei  bendras skaičius 'S'==0 ir bendras skaičius 'V'==0\n",
    "# user_type == 1, jei  bendras skaičius 'S'> 0 ir bendras skaičius 'V'==0\n",
    "# user_type == 2, jei  bendras skaičius 'S'==0 ir bendras skaičius 'V'> 0\n",
    "# user_type == 3, jei  bendras skaičius 'S'> 0 ir bendras skaičius 'V'>0\n",
    "\n",
    "result = result.reset_index()\n",
    "user_types = np.zeros(len(result), dtype = int )\n",
    "userIds = []\n",
    "for i, row in result.iterrows():\n",
    "    # print(row)\n",
    "    userIds.append(row['userId'])\n",
    "    if ((row['S'] == 0) & (row['V'] == 0)):\n",
    "        user_types[i] = 0\n",
    "    if ((row['S'] > 0) & (row['V'] == 0)):\n",
    "        user_types[i] = 1\n",
    "    if ((row['S'] == 0) & (row['V'] > 0)):\n",
    "        user_types[i] = 2\n",
    "    if ((row['S'] > 0) & (row['V'] > 0)):\n",
    "        user_types[i] = 3\n",
    "\n",
    "# print(userIds)\n",
    "\n",
    "# df_userId = pd.DataFrame(columns = ['userId', 'user_types'])\n",
    "df_userId = pd.DataFrame()\n",
    "df_userId['userID'] = userIds\n",
    "df_userId['user_types'] = user_types\n",
    "\n",
    "# print(\"\\ndf_user_Id:\\n\", df_userId)\n",
    "# print(user_types)\n",
    "# result['user_type'] = user_types\n",
    "# print(\"\\nresult\\n\", result)\n",
    "\n",
    "frac_train=0.6\n",
    "frac_val=0.2\n",
    "frac_test=0.2\n",
    "train, validation, test = split_stratified_into_train_val_test(df_userId, stratify_colname='user_types',\n",
    "                                        frac_train=frac_train, frac_val=frac_val, frac_test=frac_test, random_state=177)\n",
    "\n",
    "print(f\"frac_train: {frac_train}, frac_val: {frac_val}, frac_test: {frac_test}\")\n",
    "df_train = df_list.loc[df_list['userId'].isin(list(train['userID']))]\n",
    "print(\"\\ndf_train:\")\n",
    "print_df_list(df_train, False, True)\n",
    "train_subjcode_lst = list(df_train['SubjCode'])\n",
    "# print(train_subjcode_lst)\n",
    "file_path = Path(rec_dir, 'train_subjcode_lst.csv')\n",
    "np.savetxt(file_path, np.array(train_subjcode_lst), delimiter=',', fmt='%d')\n",
    "print(f\"\\nMokymo imties SubjCode sąraše yra {len(train_subjcode_lst)} įrašai\")\n",
    "print(\"Mokymo imties SubjCode sąrašas įrašytas į:\", file_path)\n",
    "\n",
    "df_validation = df_list.loc[df_list['userId'].isin(list(validation['userID']))]\n",
    "print(\"\\ndf_validation:\")\n",
    "print_df_list(df_validation, False, True)\n",
    "validation_subjcode_lst = list(df_validation['SubjCode'])\n",
    "# print(validation_subjcode_lst)\n",
    "file_path = Path(rec_dir, 'validation_subjcode_lst.csv')\n",
    "np.savetxt(file_path, np.array(validation_subjcode_lst), delimiter=',', fmt='%d')\n",
    "print(f\"\\nValidacinės imties SubjCode sąraše yra {len(validation_subjcode_lst)} įrašai\")\n",
    "print(\"Validacinės imties SubjCode sąrašas įrašytas į:\", file_path)\n",
    "\n",
    "df_test = df_list.loc[df_list['userId'].isin(list(test['userID']))]\n",
    "print(\"\\ndf_test:\")\n",
    "print_df_list(df_test, False, True)\n",
    "test_subjcode_lst = list(df_test['SubjCode'])\n",
    "# print(test_subjcode_lst)\n",
    "file_path = Path(rec_dir, 'test_subjcode_lst.csv')\n",
    "np.savetxt(file_path, np.array(test_subjcode_lst), delimiter=',', fmt='%d')\n",
    "print(f\"\\nTestinės imties SubjCode sąraše yra {len(test_subjcode_lst)} įrašai\")\n",
    "print(\"Testinės imties SubjCode sąrašas įrašytas į:\", file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ecg')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:18:12) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f36dab35816871602f0a4fffa6415a4e758bca001397bb3d9f7e90aab6637a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
